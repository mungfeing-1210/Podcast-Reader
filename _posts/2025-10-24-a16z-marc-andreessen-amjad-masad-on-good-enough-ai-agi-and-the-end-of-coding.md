---
layout: post
title: "A16Z · Marc Andreessen & Amjad Masad on “Good Enough” AI, AGI, and the End of Coding"
date: 2025-10-24
tags: [AI, 转写]
---

# Replit与AI编程：自然语言到代码的演进

AI技术正以前所未有的速度推动软件开发变革。本期播客将深入探讨Replit如何利用AI，让无论是编程新手还是经验丰富的开发者，都能通过简单的自然语言描述，将创意迅速转化为可运行的软件，重塑编程的未来。

## AI加速下的编程困境与期望

尽管人工智能技术在编程领域展现出惊人的进步，其发展速度超越了五年前甚至十年前的想象，但行业内部仍存在一种矛盾的心态：

*   **高速发展却仍感不足**：人们对AI编程的期望极高，认为其速度应达到计算机级别的瞬时响应，而非像人类工作般的逐步完成。
*   **担忧停滞**：市场对AI技术可能面临发展瓶颈或“停滞不前”的担忧。

这种高速发展与过高期望之间的落差，反映了业界对编程效率提升的迫切需求。

## Replit如何简化AI编程体验

Replit旨在消除编程的“偶然复杂性”，让用户专注于创意的“本质复杂性”。对于初级程序员而言，Replit与AI的结合提供了极致简化的开发流程。

*   **去除环境配置障碍**：Replit的首要目标是移除复杂的开发环境设置，让用户直接聚焦于他们的想法——“你想构建什么？”
*   **自然语言指令**：用户只需用日常英语（或日语等主流语言）描述项目构想，如“我想在线销售可丽饼”，**Replit Agent**便能理解并启动构建。
    *   **Replit Agent**：Replit内部的AI系统，用于理解用户需求、规划、构建和测试软件。
*   **智能技术栈选择**：用户无需指定编程语言或技术栈。Replit Agent会根据需求自动选择最佳方案：
    *   数据应用：选择Python和Streamlit。
    *   网络应用：选择JavaScript和Postgres。
*   **广泛的语言支持**：Replit平台已运营近十年，其基础设施支持任何主流编程语言。即使是偏好Python等特定语言的用户，也可自由选择使用。

## 编程范式演进：从机器码到自然语言

编程语言的进化史，本质上是一部不断提高抽象层次、降低门槛的民主化史。这一进程从早期开始，目标始终是让更多人能用更接近自然语言的方式表达指令。

*   **Grace Hopper的先见之明**：编译器发明者Grace Hopper早在几十年前就预言，未来人们将用“英语”编程。她认为，尽管专家需要理解底层机制，但更高级的抽象能让编程普及。
*   **Fred Brooks的复杂性理论**：软件工程的经典理论区分了**偶然复杂性（Accidental Complexity）**和**本质复杂性（Essential Complexity）**。
    *   **偶然复杂性**：指编程工具、包管理、语法等非核心技术细节，是导致业务发展受阻的“代码瓶颈”。
    *   **本质复杂性**：指解决实际问题、将产品推向市场的核心挑战。
    *   Replit及AI编程的目标，正是抽象掉偶然复杂性，让开发者直面本质问题。
*   **历史上的阻力与演进**：每次编程抽象层次的提升，都会遭遇“老派”程序员的抵触。从机器码到汇编、从汇编到高级语言（如Basic、C），再到后来的JavaScript框架（如ReactJS），这些新范式最终都成为主流，并使得更多人能够参与软件开发。AI编程是这一演进的最新一步，将输入从“语法”提升到“思想”。

## Replit Agent的工作流

在用户输入其想法后，Replit Agent会启动一个多步骤的自动化构建流程，将文字构想转化为实际应用。

*   **理解与任务列表**：Agent首先会展示它对用户需求的理解，并列出即将执行的任务清单，例如：
    *   设置数据库以存储数据。
    *   集成支付系统（如Shopify或Stripe）以处理支付。
*   **构建模式选择**：用户可以选择两种开发模式：
    *   **设计迭代**：优先进行设计方案的反复修改和确认。
    *   **全功能构建**：Agent将耗时20-40分钟，自动完成整个项目的开发。
*   **全自动构建与测试**：在选择“全功能构建”后，Agent会执行一系列复杂操作：
    *   设置数据库，执行数据迁移，编写SQL代码。
    *   构建网站并进行部署。
    *   **自动测试与修复**：最新创新是Agent 3，它能在完成代码编写后，启动浏览器进行自动测试，并循环迭代修复发现的问题。



## AI代理如何简化软件开发流程

AI代理极大简化了软件开发流程，将一个想法快速转化为可运行的应用。

*   **快速构建与迭代**：用户描述需求后，AI代理会在20-30分钟内构建出初步应用。用户可以在手机上测试，如果发现问题，只需向代理描述，代理便会自动迭代修复代码。
*   **一键发布部署**：应用完善后，用户只需几次点击即可发布。AI代理会自动在云端配置虚拟机、部署数据库等后端基础设施，形成生产环境。
*   **降低开发门槛**：相较于几年前需要手动设置本地开发环境、注册AWS账户、配置数据库和部署管道等复杂步骤，AI代理将开发门槛降至最低，即使非专业人士也能实现应用创建。
*   **开放代码可查**：对于专业开发者，Replit平台允许他们查看代理生成的代码文件，甚至通过Git推送到GitHub，或在Emacs等编辑器中打开，保持了透明度和可控性。

## AI代理：从辅助工具到核心“程序员”

AI代理的角色已从辅助工具转变为实际的“程序员”，它自主执行编程任务，而用户则成为其管理者。

*   **角色转变**：AI代理现在能够列出并**自主执行**一系列编程任务，而非仅仅按照指令行事。这意味着实际的“用户”变成了AI代理本身。
*   **地理位置案例**：Replit团队曾发现，虽然公司在亚洲设有服务器以服务亚洲用户，但当AI代理部署在美国时，亚洲用户的体验反而变差。这是因为请求需要跨洋传输给作为“程序员”的AI代理进行处理，凸显了AI代理作为独立执行者的地位。
*   **代理能力范围**：AI代理是一个软件程序，它像人类程序员一样，拥有访问和使用各种编程工具及接口的能力。
    *   **工具包括**：文件操作（创建、编辑、删除）、软件包管理（搜索、安装）、资源配置（数据库、对象存储）等。

## 提升AI代理长期推理能力的关键

AI行业面临的重要挑战是提升AI代理在复杂任务中长时间保持逻辑连贯性（coherence）的能力，即**长期推理**。

*   **早期局限**：早期的AI代理在处理复杂任务时，通常只能连贯运行2-3分钟，随后便会“失控”，陷入错误循环或产生荒谬结果。
*   **突破节点**：大约在去年（播客录制时），AI代理的连贯性突破了3-5分钟的门槛，这预示着长期推理问题正在被有效解决。
*   **长期推理定义**：指AI模型在复杂且多步骤的任务中，能够长时间保持事实和逻辑的连贯性。
*   **上下文窗口与压缩**：
    *   **上下文窗口（Context Window）**是大语言模型（LLM）的“记忆”，包含用户输入、环境信息和AI的内部思考过程。
    *   由于LLM的上下文长度有限（例如，宣称百万token，实际有效约20万token），超出后模型会开始“挣扎”。
    *   **上下文压缩技术**通过总结和提炼记忆中的信息（例如将多行日志总结为一句话）来维持上下文窗口的有效利用，从而延长模型的连贯性。

## 强化学习：AI代理实现连贯性的技术突破

强化学习（RL），特别是结合代码执行的RL，是实现AI代理长期推理能力的关键技术突破。

*   **传统预训练不足**：传统的LLM预训练（预测下一个词）虽然能让模型学习语言，但不足以有效解决长期推理问题。
*   **强化学习引入**：RL允许模型通过在编程环境中实际操作来学习问题解决。
    *   **轨迹（Trajectories）**：指模型解决问题所采取的一系列推理步骤。
*   **训练过程**：
    1.  将LLM置于编程环境（如Replit），提供代码库和带有bug的任务。
    2.  人类训练师预设了正确的解决方案（例如通过一个pull request或单元测试来验证）。
    3.  LLM会生成多个“轨迹”尝试解决问题。
    4.  只有成功解决bug的轨迹会获得奖励（reinforces），从而训练模型学习如何构建更长、更正确的推理链。

这种训练方式使LLM学会了在长步骤的编程任务中，通过试错和反馈机制，有效延伸并维持其推理链，克服了早期模型易“失控”的问题。



## 模型长程推理能力的飞跃与衡量

当前大型语言模型的长程推理能力正以惊人的速度提升。

*   **Meter基准**：非营利组织Meter设立了一个基准，用于衡量模型在保持连贯性和执行有用任务（如编程）的同时，能运行多长时间。根据他们去年发布的一份报告，模型的有效运行时间每七个月翻一番。
*   **Agent 3的实证数据**：我们发现这一增长速度被大大低估了。通过对实际用户任务（例如发布应用）的AB测试数据观察，衡量模型经济实用性的指标显示：
    *   Agent 1：能运行 **2分钟**。
    *   Agent 2（于二月发布）：能运行 **20分钟**。
    *   Agent 3：能运行 **200分钟**，部分用户甚至能推至12小时，尽管在2-3小时后性能的稳定性会下降。

## 核心突破：验证循环机制

实现模型长程推理能力飞跃的关键创新是**验证循环**（Verification Loop）机制。

### 英伟达的验证循环应用

大约七个月前，英伟达的研究发现，将验证器引入其使用DeepSeek生成GPU内核的流程中，能使DeepSeek连续运行约20分钟，并生成经过优化的内核。这证明了验证循环能显著扩展模型的工作时长和质量。

### Agent 3的多智能体系统设计

我们的公司借鉴了这一思想，投入大量时间构建了支持验证循环的脚手架，使得智能体能运行更长时间：

*   **多智能体协作**：第一个智能体执行任务约20分钟。
*   **实时测试与反馈**：第二个测试智能体会启动浏览器，模拟用户操作来验证第一个智能体的工作成果。
*   **错误纠正与新任务生成**：如果发现错误，测试智能体将总结前20分钟的工作，并结合错误报告，生成新的提示词，引导第一个智能体开启新的任务轨迹。

这种“接力赛”机制通过不断压缩前一阶段的工作成果并将其作为下一阶段的提示，理论上可以实现无限长的推理链。

## 智能体工作效率与人类程序员的对比

在运行过程中，智能体的处理速度比人类更快，但并非以纯粹的计算机速度运行。

*   **思考与反思**：智能体在处理代码差异（file diffs）时会快速推进，但每隔一段时间会停下来“思考”，反思当前路径是否正确，并审视自己的工作。
*   **工具调用**：当遇到PostgreSQL版本兼容性等新问题时，智能体会调用网络搜索工具，寻找解决方案。
*   **类人工作流**：其整个工作流程，包括推理链和测试链，像极了一个效率极高的程序员在专注工作，具有极强的观赏性。

## 从“随机鹦鹉”到逻辑推理：验证循环的历史根源

早期的大型语言模型（如两年前的ChatGPT初期版本）在处理需要理性思考和问题解决的任务时，表现出明显的局限性。

### 早期LLM的局限

*   **语言能力强，推理能力弱**：它们擅长语言的流畅性和创造性（如莎士比亚十四行诗或说唱歌词），但在数学计算（如大数相加或相乘）或简单的逻辑推理（如著名的“草莓测试”：单词“strawberry”中有几个“r”——正确答案是3个，但早期模型常答错）上能力不足。
*   **“随机鹦鹉”的批判**：当时有批评者称这些模型为“随机鹦鹉”（stochastic parrot），意指它们只是随机地重复训练数据中“听起来”正确的内容，缺乏真正的理解和推理能力。

### AlphaGo的启发与融合

LLM推理能力的突破并非全新发明，而是**2015-2016年AlphaGo突破**所代表的技术路线的复兴和融合。

*   **连接主义与符号主义的结合**：AlphaGo的成功在于融合了两种主要的AI范式：
    *   **连接主义**（Connectionism）：以神经网络为代表，擅长模式识别和生成。
    *   **符号主义**（Symbolic Systems）：强调离散推理、知识库和明确的算法规则。
*   **AlphaGo的工作原理**：
    *   神经网络生成一系列潜在的围棋走法。
    *   蒙特卡洛树搜索（Monte Carlo Tree Search）等离散算法，通过尝试和验证（即一个内置的“验证循环”），来评估并选择最佳走法。

将这种验证循环融入到现代LLM的训练和应用中，使得这些强大的生成式神经网络能够通过离散的验证方法，获得更强的数学、编程等领域的推理能力。这种训练循环的关键在于，它必须应用于一个具有**明确且可验证答案**的问题陈述。



## 可验证性：衡量AI能力的关键指标

大型语言模型（LLMs）在解决具有**明确且可验证答案**的问题时，表现出最优异的性能。

*   **医学诊断**：最佳结果需经多位医生专家组确认，或能实际治愈病症。
*   **法律论证**：有效的辩论能够使被告无罪释放。
*   **数学与物理**：需得出精确的正确答案或在现实世界中有效运行的结果。
*   **工程领域**：例如土木工程，建造的桥梁必须结构稳固，不会坍塌。

这种能力是LLMs有效推理的基础，强调了结果可量化、可验证的重要性。

## 编码：AI快速进展的“硬领域”

编码领域因其高度的可验证性，正经历AI最快的进步。

*   **即时生成与验证**：编码问题可以被迅速生成，并且其解决方案能即时得到验证。
*   **Sobbench基准测试**：
    *   **定义**：一个衡量AI软件工程任务能力的基准，由GitHub上复杂的代码库、清晰的错误描述及附带单元测试的修复拉取请求组成。
    *   **进展**：AI在该基准上的表现从2024年初的约5%迅速提升至Claude 4.5的约82%，达到了行业领先水平。
*   **代码双重验证机制**：
    *   **编译**：代码必须语法正确，能够成功编译。
    *   **正确输出**：代码执行后必须产生预期的结果。
    尽管验证正确输出更为复杂，但其明确的验证机制推动了AI在编码领域的飞速发展。

### 训练数据的高效获取

为加速AI在这些领域的进步，研究者采取了多种策略来获取高质量的训练数据：

*   **人工专家生成**：基础模型公司雇用数学家、物理学家和程序员等人类专家，来创建具有已知正确结果的训练数据。
*   **自动化生成**：开发能够自动生成训练数据、测试用例和验证结果的软件系统，即“合成数据”方法。
*   **适用领域**：这些方法在代码、数学、物理、化学、生物（如蛋白质组学）和部分机器人学等“硬领域”中效果显著。

## “软领域”的挑战与AI发展的悖论

在缺乏明确验证标准的“软领域”，AI的进展相对缓慢，这导致了对AI能力评估的复杂悖论。

*   **“软领域”定义**：指那些结果难以通过确定性、事实性且无争议方式验证的领域，例如慢性病诊断、论文写作或创意任务。
*   **抽象性**：相较于代码和数学的具象性，“软领域”的抽象特性是其进展缓慢的主要原因。
*   **AI进展悖论**：
    *   **一方面**：AI技术被誉为“有史以来最惊人的技术”，发展速度惊人。
    *   **另一方面**：公众与开发者普遍感到“失望”，认为AI进步不够快，甚至担忧其可能停滞。
这种矛盾凸显了在评估AI能力时，既要充满期待又需面对现实局限的认知张力。

### AGI路径上的不确定性

尽管当前对AGI（通用人工智能）的押注巨大，但其实现路径仍存在不确定性：

*   **主要疑虑**：当前AI系统在不同领域间缺乏显著的“迁移学习”能力。例如，编码能力的提升并不能直接带来其他领域（如医疗诊断）的同步进步，这引发了对AGI实现路径的疑问。



## 领域泛化挑战与“苦涩的教训”的再审视

人工智能在特定领域的进步，并不意味着其泛化推理能力的同步提升，这引发了对AI发展路径的深入思考。

*   **领域专精与泛化能力脱节**： AI在编码等单一领域取得显著进展，并不直接转化为生物、化学、物理、数学或法律等其他领域的通用推理能力。若要实现跨领域泛化，AI仍需针对每个新领域独立获取训练数据并构建强化学习环境。
*   **理查德·萨顿的“苦涩的教训”**： 这一理论认为，AI研究的终极路径在于利用“无限可扩展”的方法，即投入更多算力和数据，以提升性能，最终实现通用人工智能（AGI）。
*   **对“苦涩的教训”的质疑**： 在近期的一次访谈中，理查德·萨顿对该理论提出了质疑。他认为，当前AI高度依赖人类数据和标注的训练模式，可能与“苦涩的教训”所倡导的无限可扩展路径相悖。

这表明，AI社区内部对于实现AGI的有效途径仍存在分歧，尤其是在面对当前训练数据和方法论的局限性时。

## 训练数据稀缺与“功能性通用人工智能”的路径

随着互联网高质量训练数据资源的日渐枯竭，AI面临数据稀缺的瓶颈，促使研究者重新评估通用人工智能（AGI）的实现策略，转向更侧重实用价值的“功能性AGI”。

*   **数据耗尽的“化石燃料论”**： OpenAI联合创始人伊利亚·苏茨克维（Ilya Sutskever）提出，我们已基本耗尽互联网上可用于训练模型的数据。相比于从互联网“攫取”数据，生成新的训练数据既困难又昂贵。
*   **迁移学习的局限**： **迁移学习**是指机器学习系统将从一个领域习得的知识应用到另一个不同领域的能力。然而，正如人类专家（如经济学家对互联网的错误预测、爱因斯坦在政治领域的表现）往往难以将其专业知识有效地迁移到其他领域一样，AI的跨领域迁移能力也面临挑战。
*   **转向“功能性AGI”**： 鉴于数据稀缺和迁移学习的现实局限，业界开始探讨“功能性AGI”的实用价值。其核心在于收集全球所有有经济价值活动的特定数据，并在此基础上训练大型语言模型或基础模型，从而实现劳动力在特定经济部门的大规模自动化。

这一转变意味着，实现一个能够无缝泛化所有领域知识的“理想化AGI”可能仍遥远，而专注于在具体应用场景中发挥价值的“功能性AGI”更具可操作性。

## 通用人工智能（AGI）定义中的“不断移动的门槛”现象

AI科学家们普遍抱怨，通用人工智能（AGI）的定义是一个“不断移动的门槛”，使得AI的成就被低估，并导致对自身能力的过高期待。

*   **成就被重新定义**： 一旦AI掌握了某项曾被视为智能标志的任务（如战胜人类象棋高手、通过图灵测试），这项成就便不再被视为“真正的AI”，而是被归类为“计算机象棋”或“无趣的程序”。
*   **评判标准的不公**： AI工程师们抱怨，他们总是根据AI尚未解决的“下一个问题”来评判，而非其已取得的突破。图灵测试在80年间被视为AI的终极目标，但当AI真正通过时，却没有得到应有的认可和庆祝。
*   **不切实际的目标设定**： 这种对AGI的定义，设定了一个远超人类普遍能力的理想化目标，即要求AI在所有方面都超越人类，包括人类自身也很少能做到的跨领域通用能力。

这种“不公平”的评判标准，导致了AI领域的一种“自我鞭笞”现象，即科学家们设定了不切实际的目标，又因未能达到而感到沮丧。

## GPT-5 体验的“边际效益递减”

与早期版本相比，GPT-5在“可验证领域”表现出色，但在“人性化”维度上未能呈现显著进步，甚至有所倒退，预示着当前模型在某些方面可能面临边际效益递减。

*   **“人性化”维度的倒退**： GPT-5在可验证领域（如事实性知识、逻辑推理）表现良好，但处理更具“人性化”或情感色彩的任务时，进步不明显。许多用户反馈GPT-5感觉更“机械”和“机器人化”，不如GPT-4那样富有“人情味”。
*   **与早期飞跃的对比**： 从GPT-2到GPT-3，模型的“人性化”感知和对世界的理解能力有显著飞跃，让用户感觉模型“更懂自己”，能够更好地共情。
*   **整体感知能力的停滞**： 然而，从GPT-3到GPT-4再到GPT-5，这种作为“整体存在”的进步感逐渐减弱，并未像早期版本那样带来革命性的体验。

这种现象引发了对大语言模型未来发展方向的思考：是继续在可验证领域深耕，还是需要新的突破来实现更全面、更像人类的理解与交互，以克服当前的边际效益递减。



## AI在处理争议性与未解问题上的挑战

**结论**: 尽管在某些任务上表现出色，当前AI模型在面对高度争议或缺乏明确答案的问题时，其深度推理能力和创新性认知仍显不足。

*   **关键论据**:
    *   对于“新冠病毒起源”这类复杂且无定论的问题，AI（如GPT-4/5）尚未展现出解决或形成新共识的能力。
    *   早期，AI受限于**RLHF (Reinforcement Learning from Human Feedback，人类反馈强化学习)**，对于“禁忌”话题（例如，对实验室泄漏论持有偏见）会表现出回避或“说教式”回应。
    *   尽管现在一些高级模型能“**钢人化 (steel man)**”对立观点，即为某一立场构建最有力的论证，但这仍是基于现有知识的整合，而非开创性推理。

**影响**: 这类挑战提示我们，AI在帮助人类从第一性原理厘清复杂真相方面，仍有显著发展空间。

## AI在知识综合与深度分析中的卓越能力

**结论**: 相较于处理争议性问题，AI在高效整合现有知识并生成结构化深度报告方面表现出“世界级”水平。

*   **关键论据**:
    *   高级模型（如GPT-5 Pro配合深度推理或Rock 4 heavy）可按需生成30-40页的专业报告，内容涵盖任何复杂主题。
    *   例如，在分析“先进经济体征收关税时，最终由谁承担成本”这样的经济学难题时，AI能提供高度连贯、逻辑严谨且准确的分析，其质量堪比顶尖学术研究。
    *   AI的输出不仅信息量丰富，其写作风格和逻辑一致性也达到专业水平，模糊了知识综合与新知识创造的界限。

**影响**: 这种能力极大地提高了我们获取和理解复杂信息、进行高效学习的效率。

## 人工通用智能 (AGI) 的本质与未来愿景

**结论**: 对**人工通用智能 (AGI)** 的追求应超越现有AI的实际应用价值，聚焦于其“高效持续学习”的核心特质，这将是推动文明进步的关键。

*   **关键论据**:
    *   业界对AGI的定义存在分歧，传统观点常将其与“完美的人类表现”相比，如自动驾驶汽车是否是“完美司机”或“优于人类司机”。
    *   然而，DeepMind联合创始人Shane Legg等提出的AGI定义——**高效持续学习 (efficient continual learning)**——更具前瞻性。
    *   **高效持续学习型AGI的特征**：
        *   **领域通用性**：无需大量先验知识，即可在任何领域部署。
        *   **快速适应与掌握**：能像人类一样在数月内掌握复杂技能。
        *   **通用化能力**：实现广泛的技能、理解和推理能力习得。

**影响**: 这样的AGI不仅有望加深对人类意识和智能本质的理解，更将是推动人类文明进入新阶段的根本力量。虽然AI在应用层面仍有巨大潜力，但真正意义上的AGI突破仍需时日。



## 现有AI的“局部最优陷阱”

我个人对实现真正的通用人工智能（AGI）持悲观态度。因为我们现有的人工智能成果，例如大型语言模型（LLM），已经极其有用且具有巨大的经济价值。这种“足够好”的状态，反而可能成为追求更高目标的阻碍。这符合了“**Worse is Better**”原则（指一个功能简单、易于实现的系统，尽管在理论上并非最优，却可能因其实用性而占据主导地位）。我们陷入了一个**局部最优陷阱**，因为当前技术已能高效完成大量经济生产性工作，这反而降低了系统寻求通用解决方案的动力和压力。

## 通用人工智能（AGI）进展的瓶颈

尽管有Rich Sutton等少数研究者仍在探索通用智能路径，但巨大的优化投入正集中于现有局部最优方案。讽刺的是，投入数百亿美元构建的系统，很可能只是在优化一个局部最优解，而非致力于解决更通用的问题。当然，通用问题的解决可能远超我们有生之年。

### 未来研究方向与挑战

目前，除了大型语言模型（LLM）之外，很少有真正令人兴奋的新研究方向。强化学习（RL）领域的突破值得关注，特别是将生成系统与**树搜索**等技术结合，但这并非全新概念，已存在十余年。Reinforcement Learning的先驱们仍试图从零开始构建智能，例如Carmack也在探索这一路径，而非局限于LLM。然而，这一领域尚未见到显著进展或成果。当然，真正的通用智能或许已在某个角落悄然萌芽，比如一个在社交媒体上无往不胜的机器人，或是一个能自主生成卓越代码的程序。

## 编程之路：从早期探索到Replit的诞生

播客嘉宾分享了他独特的编程和创业历程。

### 童年与编程启蒙

我出生在约旦安曼。在我六岁时，身为政府工程师的父亲，在并不富裕的情况下，坚持购置了家里第一台电脑——一台IBM PC，这在当时是社区里的首例。我最早的记忆之一，就是看着父亲拆箱这台机器，翻阅厚厚的说明书，然后笨拙地输入`CD`、`LS`、`MKDIR`等DOS命令。机器精确响应命令的场景，深深吸引了我。

### 首次创业与早期洞察

我曾是个狂热的游戏玩家，常去网吧玩《反恐精英》。我发现这些网吧虽然遍布电脑，却没有任何软件来管理业务，员工仅靠手写记录。当时年仅12岁的我，主动提出为他们开发一套管理软件，历时两年完成并成功售出。这笔收入丰厚，甚至让我在13、14岁时请全班同学去当地刚开业的麦当劳大快朵颐，这是我的第一笔商业尝试。

由于当时接触科幻小说并初步了解AI，我形成了“编程即将被自动化”的认知。大学时，我因此放弃计算机科学，转向计算机工程，认为重心应转向计算机的构建与维护。此前的软件开发向导（Wizards）功能，让我看到自动化代码生成的可行性，加深了这一判断。然而，在接触Lisp、Scheme等编程语言后，我又重新找回了对编程的热爱。

### Replit的构想与实现

但当时的学习环境充满挑战。我没有笔记本电脑，每次想学习Python或Java，都得去机房下载数GB软件、配置环境、解决各种依赖问题，耗时耗力。2008年左右，Google Docs和Gmail已证明网页是理想的软件平台，我坚信所有软件都应转移到Web端。那时，我发现竟然没有一个在线开发环境，感觉就像捡到了“一张百元大钞”，于是决定自己着手开发。

我最初用几小时搭建了一个简单的原型：一个文本框，输入JavaScript代码后点击“eval”按钮即可在弹窗中显示结果。这个基础的编程环境受到朋友欢迎。然而，浏览器当时仅支持JavaScript，要实现多语言运行，需要一项技术突破。Mozilla的**Emscripten**项目（一个能将C/C++等语言编译成JavaScript的工具）提供了解决方案。我率先将CPython编译成JavaScript，让浏览器也能运行Python代码。随后，我们团队将Ruby等多种语言也移植到浏览器中。**Replit**这一名称便由此而来，意指提供一个**REPL**（Read-Eval-Print Loop，交互式编程环境）来运行各种编程语言。我们将这些底层基础设施开源，项目随后迅速传播。



## 开源项目奠定赴美基础

作者早年致力于构建浏览器内代码运行的基础设施，该项目在 Hacker News 上广为传播。这一时期，正是大规模在线开放课程（MOOC）如 Udacity、Coursera 和 **Code Academy** 兴起的时代。

*   **Code Academy**：作为首个提供浏览器内交互式编程学习的网站，其大量核心功能构建在作者的开源软件之上。
*   **合作邀约**：Code Academy 团队在发现作者的贡献后，主动联系并表达了招聘意向。
*   **初期拒绝**：作者当时正酝酿创办自己的公司 Replit，因此拒绝了全职工作机会。
*   **赴美契机**：Code Academy 团队多次邀请，并最终提供了一份“无法拒绝”的入职条件，包括 O1 签证，促使作者决定移居美国。

## 学业困境催生黑客行为

因对冗长学业的不满和投身科技创业的强烈渴望，作者在大学时期产生了修改成绩、加速毕业的想法。

*   **硅谷之梦**：作者早在1998/99年观看电影《硅谷传奇》时，便萌生了未来移居美国、投身科技界的梦想。
*   **学业停滞**：尽管编程能力出众且充满创业想法，作者因大学课程内容枯燥、长期缺勤，导致学业停滞长达六年之久，远超正常学制，内心备感沮丧。
*   **多相睡眠实验**：为高效利用时间进行黑客攻击，他尝试实行了“**多相睡眠**”（Polyphasic Sleep），即每隔数小时小憩20分钟，以配合运行耗时较长的入侵脚本。
*   **入侵尝试**：经过两周的努力，作者成功利用 **SQL 注入**漏洞渗透大学数据库，最终获得了对**主数据库**的访问权限，并修改了自己的成绩。

## 黑客事件的意外转折与和解

作者修改成绩的行为导致大学系统崩溃，最终揭示了系统深层漏洞，也为他的人生带来了意想不到的转折。

*   **系统故障**：作者的成绩记录中出现了“通过考试”与“被禁止参加考试”的逻辑冲突，导致大学系统全面瘫痪。
*   **数据库缺陷**：问题根源在于大学数据库未进行**范式化**设计（数据结构冗余，更新时易产生异常），一个禁止考试的布尔值标志未被正确重置，导致了系统崩溃。
*   **坦白与讲解**：作者选择坦白，在计算机系所有院长面前，于白板上详细演示了漏洞原理和攻击方法，赢得了一众技术人员的认可。
*   **校长介入**：事件最终上报至大学校长，校长引用“**能力越大，责任越大**”勉励作者，并决定给予他第二次机会。
*   **和解条件**：作为和解条件，作者需要在暑期协助系统管理员加固系统安全，但实际工作中并未获得协作。
*   **毕业项目**：在毕业设计中，他被迫接受了主任提出的安全与黑客主题，开发了一款**安全扫描器**。
*   **再现漏洞**：这款扫描器在演示时，意外地再次发现了大学系统中的另一个安全漏洞。



## 揭露系统漏洞：一场意外的“政治秀”

### 关键发现与现场对峙
在一次系统功能展示中，我的程序意外检测出安全漏洞，并成功获取了系统 shell（命令行访问权限）。然而，一位负责系统安全的教务长坚决否认存在漏洞，甚至质疑我“撒谎”。

*   **漏洞自动揭示**：系统演示时，自动识别出安全漏洞并获取了 shell。
*   **教务长否认**：负责系统安全的教务长面红耳赤，声称系统已安全，不可能存在漏洞。
*   **现场验证**：应教务长要求，我尝试查询其密码。
    *   最初显示为加密乱码，教务长以此为据，再次指责我“撒谎”。
    *   我利用程序内置的解密功能，当场揭示了其明文密码（内容令人尴尬）。
*   **权力斗争背景**：事后我才意识到，自己可能被另一位教务长利用，成为其旨在公开羞辱安全负责人的棋子。

### 结果与启示
这次事件促使学校最终修复了系统漏洞，我也得以顺利毕业，并将相关软件交予学校。它让我首次认识到，即使在学术环境中也存在复杂的职场政治。

## AI时代的独立思考与路径选择

### 传统路径的挑战
面对当今快速发展的世界，特别是AI时代的到来，我认为仅仅遵循传统、墨守成规的发展路径，其回报正变得越来越低。

*   **收益递减**：传统、顺从的路径，其价值和回报正在逐渐减少。
*   **利用工具探索**：鼓励年轻人充分利用现有工具，自主探索并规划自己的独特道路。
*   **拒绝盲从**：不应再盲目听从传统建议，或简单重复前人走过的路。

### 自主开创的价值
在AI时代，个体更应培养批判性思维和创新精神，勇于打破常规，开创符合自身优势和时代趋势的全新道路。
