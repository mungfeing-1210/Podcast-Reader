---
layout: post
title: "上下文学习、预训练与人类智能的类比"
date: 2025-10-20
tags: [AI, 转写]
---

以下是将原文改写为高质量中文Markdown文章的内容：

---

## 强化学习的挑战与智能体发展的十年展望

### 引言：对强化学习与当前AI态势的看法

强化学习（Reinforcement Learning）作为一种技术，在发展过程中面临着诸多挑战。尽管如此，相较于此前的一切方法，其表现已显著优越。我个人对强化学习持乐观态度，认为它是可行且有望成功的。之所以有时听起来略显悲观，是因为我在Twitter上看到许多令人费解的内容。其中很大一部分，坦白说，我认为是为了募资。

我们并非在真正构建“生物”，而是在创造“幽灵”——这些飘渺的数字实体，它们完全是数字化的，并模仿人类行为。这代表着一种不同类型的智能。当前正处于一场持续了数十年的“智能爆炸”之中，自动化正逐步渗透到各个领域，这一过程已经持续了数百年。因此，与其撰写博客、制作幻灯片，不如专注于**构建代码、组织代码并使其运行**。这是唯一的前进之路，否则我们将错失宝贵的知识。

设想一下，如果有一个完美的AI导师，我们或许能够取得惊人的进展。然而，当今的天才们，也仅仅是触及了人类思维潜能的皮毛。

### 访谈：智能体（Agents）的十年与瓶颈

今天，我有幸与Andre Karpathy对话。

**问：** Andre，您为何认为当前将是“智能体的十年”，而非“智能体之年”？

**Andre Karpathy：** 谢谢邀请。您提到的“智能体的十年”这一说法，实际上是对业界此前一种观点的回应。我不太确定具体是谁提出的，但当时许多实验室都在暗示，凭借大型语言模型（LLMs）及其发展，今年将是“智能体之年”。

这种说法触动了我，因为我感到行业内存在一些过度预测。在我看来，更准确的描述应该是“智能体的十年”。我们确实拥有一些非常早期、令人印象深刻且我日常使用的智能体，例如Claude和Codex。然而，我仍然觉得还有大量工作亟待完成。因此，我的观点是，我们将在未来十年内不断改进这些技术，它们会变得越来越好，最终带来巨大的变革。我只是对预测的时间线持保留态度。

**问：** 那么，您认为哪些方面需要十年才能实现突破？瓶颈何在？

**Andre Karpathy：** 简单来说，是让它们真正运作起来。在我看来，无论是实验室还是我个人，在谈论“智能体”时，我们将其想象成可以雇佣的员工或实习生。试想一下，您何时会选择让Claude或Codex这样的智能体来完成您员工的工作？显然，目前它们还无法做到。要让它们能够胜任，需要克服哪些障碍？为何今天我们还不能这样做？

原因在于它们目前还无法正常工作。它们缺乏足够的智能，多模态能力不足，无法进行复杂的计算机操作等等。它们还有很多不足之处，例如不具备持续学习能力——您无法告诉它们一件事，指望它们永远记住。它们在认知上存在缺陷，无法有效地完成任务。我认为，解决所有这些问题大约需要十年时间。

**问：** 很有趣。作为一名专业播客主持人，同时也是AI领域的旁观者，我很容易识别出哪些方面是缺乏的，比如持续学习或多模态能力。但我很难对这些缺失设定时间表。比如，如果有人问“持续学习需要多久才能实现？”我没有先验知识来判断这是一个5年、10年还是50年的项目。

**问：** 为什么是十年？而不是一年或五十年？

**Andre Karpathy：** 嗯，这其中包含了我的个人直觉，以及我对自身在该领域经验的推断。我在AI领域工作了近二十年，大约15年左右。当然，Richard Sutton教授比我更资深。在这15年间，我见证了许多预测的提出与实际结果，也曾在工业界和研究领域工作过。因此，我形成了一种普遍的直觉。

我认为这些问题是**可处理的**、**可克服的**，但它们依然**十分困难**。如果将这些经验平均来看，十年似乎是一个合理的估计。

### AI发展历程中的“地震式转变”

**问：** 这非常有趣。我不仅想了解历史，更想知道在各个突破性时刻，在场的人们对即将发生的事情有何感受？他们的感受是过于悲观还是过于乐观？

**Andre Karpathy：** 是的，我们可以逐一探讨。这是一个宏大的问题，毕竟涉及15年间的诸多事件。AI之所以如此精彩，在于它经历了数次**“地震式转变”**，每次转变都让整个领域的面貌焕然一新。我大概亲身经历了其中两到三次。我仍然认为未来还会出现更多这样的转变，因为它们总是在意想不到的时刻到来。

#### 1. 深度学习：从利基到主流

我的职业生涯始于深度学习，当时我对深度学习产生兴趣，很大程度上是因为我在多伦多大学恰好在Jeff Hinton身边。Jeff Hinton无疑是AI领域的“教父级”人物，他当时正在训练这些神经网络，我觉得这既不可思议又引人入胜。然而，深度学习绝非当时AI领域的主流。它只是一个**小众的旁支科目**。

而第一次“地震式转变”——一种戏剧性的重大转变，我认为是随着**AlexNet**等的出现而发生的。

#### 2. AlexNet与任务导向AI的兴起

AlexNet的出现，可以说重新调整了整个行业的方向，每个人都开始训练神经网络。但那时的模式仍然是非常**针对特定任务**的。例如，我可能有一个图像分类器，或者一个神经机器翻译器。

随后，人们开始非常缓慢地对**智能体（agents）**产生了兴趣。大家开始思考：“好的，我们可能在视觉皮层等领域取得了进展，但大脑的其他部分呢？我们如何才能获得一个真正的、完整的智能体，一个能够与世界互动的实体？”在我看来，大约在2013年左右，**Atari深度强化学习**的兴起，就是早期智能体探索的一部分。它试图让智能体不仅能感知世界，还能采取行动、进行互动，并从环境中获取奖励。当时，这些环境就是Atari游戏。

#### 3. Atari与强化学习的“误判”

我个人觉得，那其实是一个**误判**。甚至我早期参与的OpenAI也采纳了这一方向，因为当时的主流思潮就是强化学习环境，即通过玩游戏、击败游戏、获取各种游戏来学习。OpenAI也做了很多这方面的工作。所以，这可以说是AI领域的另一个显著阶段，持续了两三年，大家都在研究基于游戏的强化学习。

但我始终对游戏能否真正导向通用人工智能（AGI）持怀疑态度。在我看来，我们追求的应该是像会计师那样，或能与**真实世界**互动、执行**知识工作**的智能体。我当时不认为游戏能实现这个目标。

因此，我在OpenAI的一个项目，作为Universe项目的一部分，就是开发一个能够通过键盘和鼠标操作网页的智能体。我非常希望能有一个能与数字世界互动、执行知识工作的系统。

#### 4. OpenAI Universe项目的早期尝试与教训

然而，事实证明，这个项目启动得**极其超前，为时过早**。早到我们本不该在那时开展这项工作。因为如果你只是漫无目的地尝试，盲目地敲击键盘、点击鼠标，试图在这些环境中获取稀疏的奖励，你是无法有效学习的。你会耗费大量的计算资源，却一无所获。

当时我们所缺失的，正是**神经网络的表征能力**。

#### 5. 大语言模型（LLM）带来的范式转变

举例来说，今天人们也在训练那些使用计算机的智能体，但它们都是**基于大型语言模型**构建的。这意味着，你必须首先拥有一个语言模型，首先获得这些**表征**，而这正是通过所有预训练和大型语言模型技术来实现的。

所以我感觉，或许可以这样说，人们曾多次尝试过早地构建完整的智能体，无论是Atari项目、Universe项目，还是我个人的经验。但实际上，在真正迈向这些智能体之前，你需要先完成一些基础工作。

如今的智能体确实更加强大了，但可能我们仍然缺少整个技术堆栈中的某些部分。然而，我总结AI发展主要分为三个阶段：

1.  **为特定任务训练神经网络。**
2.  **早期尝试构建智能体（例如Atari和Universe），但为时过早。**
3.  **开发大型语言模型，并真正寻求神经网络的表征能力**，为后续的智能体发展奠定基础。

---

以下是将原文改写为高质量中文Markdown文章：

## AI发展路径探讨：从神经网络的表示能力到动物智能的启示

### 深度神经网络的基石与后续发展

最初的思考集中于代理（agents）以及大型语言模型（LLMs）的初步应用。关键在于深入挖掘神经网络的表示能力，在添加其他复杂功能之前，充分理解并利用其内在的强大力量。

### 突发式视角：人类与动物的自然学习模式

有人提出了一种“突发式”（sudden perspective）的观点，认为人类，乃至动物，能够一次性地处理和理解周围的一切。例如，动物在没有语言支架或预设标签的情况下，被投入世界后，能立刻开始感知并理解一切。

这种视角设想AGI（通用人工智能）的愿景应当是：它能够仅凭感官数据，观察计算机屏幕，然后从零开始理解和推断事物的运作方式。就像人类或动物的成长过程一样，它们并非经过“数百万年”的训练，而是通过这种自然的方式学习。

### 对动物类比的质疑：演化与内置硬件

然而，对于将AI与动物进行类比，需要持谨慎态度，因为动物是通过截然不同的优化过程演化而来的。

*   **内置硬件的巨大优势：** 动物天生就带有大量的“内置硬件”。例如，斑马出生几分钟后就能奔跑并跟随母亲，这是一个极其复杂的行为，并非通过强化学习（Reinforcement Learning）获得，而是“内置”在基因中的。
*   **演化对神经网络的编码：** 演化显然以某种方式通过基因（ATCGS）编码了动物神经网络的权重。尽管我们尚不清楚其具体机制，但它确实有效。

### AI的“幽灵”实体：非演化训练的产物

由于我们并未运行与动物相同的演化过程，因此在AI领域直接借鉴动物智能是值得商榷的。

*   **“幽灵”而非“动物”：** 我们并非在构建动物，而是在创造“幽灵”或“精神实体”。当前的AI训练主要通过模仿人类行为和互联网数据进行，而非通过演化。
*   **数字化的模仿者：** 这些AI是完全数字化的、模仿人类的“精神实体”，代表着一种不同的智能形式。虽然我们并非构建动物，但理论上可以随着时间推移，使它们变得更具动物性，而这正是我们应当努力的方向。

### Sutton框架与强化学习的局限性

Sutton的框架旨在构建“动物式”智能，即通过单一算法从互联网上学习一切。如果能实现，那将是了不起的成就。然而，这不太可能存在，因为动物本身也并非如此学习。

*   **演化的外循环：** 动物的学习依赖于演化的外循环。
*   **大脑的成熟而非纯粹学习：** 动物的许多“学习”表现实际上是大脑的成熟过程。真正的强化学习在动物身上更多地体现在运动任务上，而非智力任务。人类在解决问题等智力任务中，也可能很少使用强化学习。

### 演化：知识的算法而非知识本身

**关于演化与预训练的类比：**
有人提出，演化类似于预训练，它构建了一个能够理解世界的系统。区别在于，演化是通过3GB的DNA实现的，它并非直接编码大脑中的每一个突触，而是更像在寻找一种能够进行终身学习的“算法”。

**对“压缩”和“学习算法”的认同：**
确实存在某种“奇迹般的压缩”和“内嵌的学习算法”，它们在生命周期中进行在线学习。

**实用主义视角下的“糟糕演化”：**
从实用主义角度来看，我们当前无法复制演化过程。然而，通过模仿互联网文档，我们可以构建出“幽灵般的精神实体”，它们具备某种内置知识和智能，这与演化的作用有相似之处。
我们将这种预训练称为“糟糕的演化”（crappy evolution），它是在现有技术条件下，达到一个初步起点，从而能够进行强化学习等后续操作的实用方法。

### 预训练的双重作用：知识与智能算法

进一步探讨预训练的本质，它实际上 выполняя两个相对独立的功能：

1.  **知识获取：** 它从互联网上吸收大量“知识”。
2.  **智能涌现：** 通过观察互联网上的算法模式，它能启动神经网络内部的各种“小电路和算法”，从而实现像语境学习（in-context learning）等能力，使其变得智能。

### 知识的潜在阻碍与认知核心的未来

令人意外的是，预训练所获得的“知识”有时反而可能阻碍神经网络的发展，使其过于依赖现有数据，难以跳出数据流形（data manifold）进行创新。

**未来研究方向：认知核心（Cognitive Core）**
未来的研究范式之一，应是寻找方法去除部分知识，以保留一个“认知核心”。这个核心是一个智能实体，它剥离了外部知识，但包含了智能的算法、解决问题的策略以及“魔法”所在。

### 语境学习：智能的可见体现

语境学习（In-Context Learning, ICL）是模型展现出最显著智能的场景。当我们与模型互动时，如果它能表现出思考、自我修正等行为，那正是其智能的可见体现。

*   **ICL的形成机制：** 语境学习过程是通过预训练中的梯度下降来发展的，模型通过元学习（meta-learning）自发地掌握了语境学习能力。
*   **ICL与演化的类比：** 语境学习本身并非梯度下降，这类似于人类的终身智能学习能力受演化条件限制，但实际的终身学习过程是通过其他机制发生的。

（注：对此类比的完整认同度存在分歧）

以下是原文的Markdown改写版本：

# 上下文学习、预训练与人类智能的类比

## 一、上下文学习与梯度下降机制的探讨

对于“上下文学习（In-context Learning, ICL）未进行梯度下降”的观点，我并不完全认同。虽然ICL并非执行*显式*的梯度下降，但我仍认为其机制与梯度下降存在某种关联。

### 1. ICL的本质：模式补全

在上下文中，ICL本质上是在一个令牌窗口内进行模式补全。模型通过其神经网络的权重来发现并完成这些模式。这种模式发现和补全的过程本身，某种程度上可以被视为一种适应性行为。

### 2. ICL中潜在的内部梯度下降循环

一些有趣的研究论文深入探讨了ICL的内在机制，并指出ICL可能在神经网络的层中运行着一个微小的内部梯度下降循环。

*   **线性回归案例：**
    *   一个具体的例子是使用ICL进行线性回归。模型的输入是`XY`对，然后当输入`X`时，模型能准确预测`Y`。
    *   通常，线性回归会通过一个小的梯度下降优化器来计算权重梯度并进行更新。
    *   研究人员发现，当他们分析ICL算法的权重时，确实找到了与梯度下降机制的类比之处。
    *   甚至有论文更进一步，通过注意力机制和神经网络的内部结构，**硬编码**了神经网络的权重使其执行梯度下降。

### 3. ICL与梯度下降的推测

因此，我推测上下文学习很可能在内部执行着某种“奇特的”梯度下降操作。虽然我们不完全了解其具体工作原理，但这种可能性是存在的。这表明，上下文学习与预训练一样，可能都涉及某种形式的梯度下降。

## 二、信息存储与感知差异：预训练与上下文学习的对比

如果上下文学习和预训练都包含了某种梯度下降，那么为何上下文学习给人的感觉更像是“持续学习”或“真实智能”，而预训练却不具备这种直观感受？关键在于模型存储和同化信息的方式与效率。

### 1. 信息同化效率的巨大差异

*   **预训练（长期记忆）:**
    *   以Llama 3为例，它在15万亿个令牌上进行了训练，70B模型的信息同化效率约为每个令牌0.07比特。
    *   这意味着知识经过了巨大的压缩，存储在模型权重中的信息就像是对互联网文档的“模糊回忆”。
*   **上下文学习（工作记忆）:**
    *   在上下文学习中，KV缓存随着每个附加令牌的输入而增长，其信息存储量约为每个令牌320千字节。
    *   这表示上下文学习的信息同化效率是预训练的3500万倍。

### 2. 类比：模糊回忆与工作记忆

我倾向于将神经网络训练过程中存储的知识比作一种“模糊的回忆”。这种模糊性源于训练过程中发生的剧烈压缩：将15万亿个令牌压缩成只有几十亿参数的最终网络。

相反，在上下文窗口中发生的一切，包括KV缓存的建立，都代表着模型的“工作记忆”，这些信息可以直接被神经网络访问。

*   **实例：** 当你向LLM询问某本书的内容时，如果仅仅依赖其预训练的知识，它可能会给出大致正确但细节不足的回答。但如果你将完整的章节提供给模型作为上下文，它就能给出更准确、更深入的答案，因为它已将信息加载到其工作记忆中。

## 三、LLMs与人类智能的差距：缺失的大脑部件

我们尚未完全复制人类智能的许多方面。尽管Transformer神经网络非常强大和通用，我倾向于将其类比为大脑的“皮质组织”。皮层以其极强的可塑性而闻名，可以被重新布线并适应不同的功能。

### 1. LLMs现有能力的类比

*   **Transformer神经网络：** 类似于“皮质组织”，其强大、通用和适应性强的特点与皮层相似。
*   **推理与规划（思维模型）：** 类似于“前额叶皮层”，负责复杂的认知功能。

### 2. 缺失的“大脑部件”

然而，仍有许多大脑区域和核团在LLMs中尚未被探索或复制：

*   **基底神经节 (Basal Ganglia):** 可能与模型微调时的强化学习有关，但目前尚不明确。
*   **海马体 (Hippocampus):** 存储记忆的关键区域，在LLMs中尚未有明显的对应。
*   **小脑 (Cerebellum):** 虽然传统认为与认知关联较小，但其作用仍有待探索。
*   **杏仁核 (Amygdala):** 负责情绪和本能，这是LLMs目前完全缺失的部分。
*   **其他古老的脑核团：** 许多在进化上更为古老的脑区，其功能和影响尚未在LLMs中得到体现。

### 3. LLMs的认知缺陷

我并不认为我们应该致力于构建人类大脑的精确模拟，因为我更倾向于从工程角度思考问题。但目前LLMs的“认知缺陷”是显而易见的。它们还无法被视为一个合格的“实习生”，因为许多关键的“大脑部件”尚未被“打勾”实现。

## 四、持续学习的自发涌现与“睡眠”机制

有人认为，持续学习（超越单一会话的长期记忆）会像上下文学习一样，随着模型的激励（例如，通过包含多个会话的外部循环强化学习）而自发涌现。

### 1. 对自发涌现的质疑

我对此观点持保留态度，因为现在的模型在每次启动时，上下文窗口都是从零开始的，它们每次都像“从头开始”。

### 2. 人类“睡眠”机制的缺失

我将人类的清醒状态比作构建一个上下文窗口，收集一天中发生的信息。但当我们入睡时，一个神奇的过程发生了：那些上下文窗口中的信息并没有直接保留，而是经历了一个“蒸馏”过程，融入到我们大脑的权重中。

大型语言模型目前缺乏这种将短期上下文信息“蒸馏”到其长期权重中的等效机制。这种“蒸馏”过程对持续学习至关重要，而这正是当前LLMs所欠缺的。

以下是将原文改写为高质量中文Markdown文章的内容：

## 论大型语言模型中的蒸馏与持续学习

### 当前模型的局限性

目前大型语言模型（LLMs）缺乏一个与人类学习过程中知识“蒸馏”阶段相对应的机制。这个“蒸馏”阶段对于持续学习（continual learning）至关重要。具体而言，现有模型尚未真正具备以下能力：

*   **分析与反思：** 对已发生的事进行深入分析和反思。
*   **合成数据生成：** 能够通过一种合成数据生成过程，将分析结果蒸馏回模型权重中。
*   **个性化适应：** 缺乏为特定个体定制的神经网络，例如通过LoRA（低秩适应）技术调整部分稀疏权重，而非修改完整的神经网络权重。

### 人类学习机制的启示

我们希望创建出具有“超长上下文”的AI个体，这不仅仅是依赖于不断增长的上下文窗口，更重要的是能将知识有效“蒸馏”到模型的权重中。人类的学习机制为此提供了重要启示：

*   **知识蒸馏：** 人类具备将知识内化并固化到其认知“权重”中的能力。
*   **稀疏注意力机制：** 人类可能拥有某种复杂而稀疏的注意力机制，使其能够高效处理大量信息。

### 未来发展方向

我们正逐渐看到一些模型开始探索稀疏注意力机制，例如DeepSE v3.2就提供了稀疏注意力的示例，这有助于实现超长上下文窗口。从某种意义上说，人工智能的发展似乎正在重新实现生物演化过程中出现的认知技巧，并有望在认知架构上趋于一致。

## 未来十年AI架构展望

### Transformer架构的演变

**提问：** 在未来十年，AI模型是否仍将是Transformer的变体，但具有更强的注意力机制和更稀疏的MLP（多层感知器）等？

**回应：** 我们可以从“时间平移不变性”的角度来思考。回顾十年前（2015年），当时主要流行的是卷积神经网络（CNNs），残差网络（ResNets）刚刚问世，Transformer模型尚未出现，许多现代的Transformer改进也未曾有过。

### 历史对比与趋势预测

展望未来十年，我认为有一些核心趋势将保持不变：

*   **核心范式：** 我们仍将通过正向传播、反向传播和梯度下降来训练巨型神经网络。
*   **规模增长：** 模型的一切都将变得更大。
*   **架构演变：** 虽然核心训练范式不变，但具体的模型架构可能会有所不同。

## AI发展要素的综合进步

### 复现LeCun 1989卷积网络

为了探究AI进步的驱动因素，我曾尝试复现Yann LeCun于1989年提出的卷积网络。这是我所知的第一个通过梯度下降训练的现代神经网络，用于数字识别。通过这项实验，我得以分析算法、数据和计算系统在进步中的贡献。

*   **算法进步：** 仅凭算法进步，例如将学习率减半（相当于33年的算法演进），就能将错误率减半。
*   **数据需求：** 要取得更进一步的提升，需要增加大量数据，例如将训练集扩大10倍。
*   **计算与优化：** 此外，还需要更多的计算优化，包括更长时间的训练，以及使用Dropout等正则化技术。

### 核心发现：多要素协同提升

实验表明，AI的进步并非单一因素驱动，而是多方面同时改进的结果：

*   **数据量：** 拥有更多的数据。
*   **硬件性能：** 拥有更好的硬件。
*   **软件与内核：** 拥有更好的内核和软件。
*   **算法优化：** 拥有更好的算法。

所有这些要素都必须同步提升，它们之间没有哪一个能一枝独秀，反而贡献都出奇地均衡。

### 长期趋势预测

这一趋势已经持续了一段时间。因此，未来十年，我预计算法上会有所不同，但一些长期存在的底层技术（如使用梯度下降训练巨型神经网络）可能仍会是主流。虽然半数错误率听起来进步不大，但实际上已经非常显著。令人震惊的是，要实现这种进步，几乎所有方面都需要全面提升。

## `nanoGPT`项目经验与学习方法

### `nanoGPT`的目标与价值

`nanoGPT`是一个开源代码库，旨在提供一个最简单、完整的、端到端的GPT克隆构建流程。它不仅展示了各个独立步骤的实现，更重要的是，它将整个管道整合在一起，为学习者提供了清晰的指导。

### 最佳学习路径：从零开始构建

在构建`nanoGPT`的过程中，我个人并没有学到太多全新的东西，更多是机械地实现已知概念并使其代码足够清晰。对于学习者而言，最好的学习方式是：

*   **并行学习：** 将`nanoGPT`代码库放在一个显示器上作为参考。
*   **手动复现：** 在另一个显示器上从头开始构建代码，不允许复制粘贴，只能参考其实现逻辑。

需要注意的是，`nanoGPT`本身是一个庞大的代码库。其最终形态并不能完全展现代码的构建过程，即如何从小的代码块开始逐步扩展和整合。未来，我计划通过视频等形式补充这一“代码块增长”的构建过程。

### 实践出真知：深度理解的关键

通过从零开始构建，学习者会被迫面对自己不真正理解的地方，从而获得更深层次的理解。正如一句代码名言所说：“如果我无法构建它，我就不理解它。”

*   **避免表层知识：** 仅仅阅读博客文章或幻灯片，往往只能获得表层知识。
*   **动手实践：** 只有亲手构建代码，将其组织并使其运行起来，才能真正掌握知识。否则，你必然会遗漏某些关键环节。

### 编程模型（LLMs）的辅助作用分析

在开发`nanoGPT`过程中，编码模型（LLMs）对我的帮助非常有限。

*   **当前三种编码交互模式：**
    1.  **完全拒绝：** 完全不使用LLMs辅助编程。这种方式现在可能不再是最佳选择。
    2.  **中间模式（我的方式）：** 仍然大量手写代码，但会利用LLM的自动补全功能。大部分时候自动补全的结果是正确的，偶尔需要编辑。在这种模式下，开发者仍然是代码的架构师。
    3.  **代理模式（VIP编码）：** 直接指令LLM完成特定任务（例如“请实现某个功能”）。这种模式下，LLM作为代码代理。

*   **代理模式的应用场景：** 代理模式在某些特定场景下非常有效，例如生成样板代码（boilerplate code）或重复性代码。

*   **工具选择：** 各种LLM工具各有优劣，关键在于学习何时以及如何有效地利用它们。

以下是原文改写后的Markdown版本：

## AI模型在代码生成中的表现：优势与挑战

AI模型在某些特定类型的代码生成方面表现出色，但在处理高度独特或复杂的代码时则面临显著挑战。

### 擅长复制粘贴式代码

AI模型非常擅长处理“样板代码”（boilerplate code），即那些通过复制粘贴即可重复使用的代码。它们也善于生成互联网上频繁出现、训练集中有大量示例的代码。这使得模型在处理这些常规任务时表现优异。

### 对`nanohat`项目的局限性

然而，对于像`nanohat`这类具有高度独特性和复杂性的项目，AI模型则显得力不从心。`nanohat`的代码并非样板代码，其结构经过精心设计，知识密集且精确排列，这使其成为一个相当独特的代码仓库。

### 模型认知缺陷与误解

AI模型存在诸多认知缺陷。它们常常误解代码，这主要是因为模型从互联网上的典型编程模式中学习了过多的“记忆”，而`nanohat`并未采用这些典型模式。它们总是试图按照常见的、规范化的方式来处理代码。

## 案例分析：梯度同步机制的冲突

一个具体的例子是`nanohat`项目中涉及八个GPU之间梯度同步的实现。

### 模型推崇的常规做法

在PyTorch中，同步梯度通常采用分布式数据并行（DDP）容器，它能自动处理反向传播过程中的通信和梯度同步。

### `nanohat`的定制化方案

然而，在`nanohat`中，作者并未选择使用DDP，认为其并非必需，因此将其移除。取而代之的是，作者在优化器的步长（step）中编写了自定义的同步例程。

### 模型对定制代码的理解障碍

模型未能内化和理解这种自定义的实现。它们反复尝试引导作者使用DDP容器，因为它们认为作者应该遵循“普通代码”的编写方式，而无法接受或适应这种非标准但有效的解决方案。

## 代码质量与风格问题

除了对代码逻辑的误解，AI模型在代码风格和质量方面也引入了问题。

### 引入冗余与复杂性

模型倾向于过度防御性编程，例如大量添加`try-catch`语句，试图将代码库打造为“生产级”。然而，这导致代码库臃肿，增加了不必要的复杂性。作者的代码中包含一系列预设假设，这些防御性机制实际上是多余的。

### 误解及使用弃用API

模型不仅误解代码，还频繁使用已弃用的API，使得生成的代码“一团糟”。

### 手动优于AI生成

总而言之，模型生成的代码实用性不高。虽然可以对其进行清理，但这并不会比直接手动编写更有效率。

## 高效编程交互方式探讨

关于人与代码生成模型交互的效率，作者提出了一种更优的方式。

### 打字式输入的低效

用英语详细描述所需代码的需求往往效率低下，因为这涉及大量的打字工作。

### 代码位置与自动补全的高效结合

作者认为最高效的信息带宽交互方式是：直接导航到代码的特定位置，输入前几个字母，然后利用自动补全功能来完成代码。这种方式能够非常精确且快速地表达意图。

## AI模型的实际应用场景

尽管存在上述局限，AI模型在某些特定场景下仍能发挥积极作用。

### 样板报告生成

作者曾使用模型生成报告，由于这类报告具有较强的样板性质且不涉及任务关键型代码，模型的表现令人满意。

### 辅助不熟悉语言开发（Rust tokenizer）

在用Rust语言重写tokenizer时，由于作者对Rust相对不熟悉，模型提供了有效的帮助。在Python实现作为充分理解和测试基础的前提下，模型能够辅助编写更高效的Rust版本。

### 提升语言学习可访问性

模型能够降低学习不熟悉语言或编程范式的门槛。由于互联网上存在大量的Rust代码，模型在这方面表现得相当出色。

## AI与编程生产力的演进：长远视角

AI在编程领域的应用，引发了关于其对人类生产力乃至超级智能发展影响的深刻讨论。

### AI工程与超级智能的争议

关于AI在自动化、AI工程和AI研究领域的爆炸性发展将迅速催生超级智能的观点，是当前主要论点之一。这种观点认为，如果AI能够从零开始创建整个应用程序，那么它将能以前所未有的速度发现架构优化。

### 模型的根本局限：原创性代码

然而，作者的经验表明，模型在生成“从未被编写过”的、需要原创性思考的代码方面表现不佳。这与“AI爆炸论”中预测的、AI能迅速实现突破性发现的图景形成对比，暗示了当前AI在创造性代码方面仍有显著限制，这可能也是作者对短期内AI爆发式发展持谨慎态度的原因。

### AI作为计算的自然延伸

作者认为，AI不是一个全新的独立概念，而是计算技术发展的一个根本性延伸。它与编程史上提高程序员生产力的各种工具，如编译器、Linter、更优的编程语言、代码编辑器、语法高亮、类型检查甚至搜索引擎等，构成一个连续统一体。

### “自动化滑块”：人类角色与抽象层级的提升

编程的历史是一个不断提高抽象层次、减少低级任务的过程。从手工编写汇编代码到使用高级语言和编译器，人类逐渐将底层细节交给机器处理。AI的出现，进一步推动了这个“自动化滑块”，使人类能够进一步从低层次的工作中解放出来，专注于更高抽象层面的问题解决。

## 对`Nanohat`架构集成的困境

对于模型为何难以完全理解并集成`Nanohat`的独特架构调整，作者给出了进一步的解释。

### 模型对已有知识的有限理解

尽管`Nanohat`的架构调整可能已在论文或代码库中公开，模型似乎“有点知道但又不完全知道”。它们难以将这些信息完全融入到特定代码库的独特风格、惯例、自定义功能以及其背后的假设中。模型尚未达到能真正理解、集成并使其合乎逻辑的程度。

### 当前模型能力的现状与行业宣传

作者承认模型能力仍在持续改进，例如GPT-5 Pro等最新模型已非常强大，在某些问题上能提供令人惊讶的良好答案。然而，作者也批评当前行业对AI能力的宣传可能过于夸大，尚未真正达到“惊人”的程度，可能存在为了融资或其他目的而夸大其词的现象。目前，模型仍处于一个“中间阶段”，仍需大量工作。

## 个人结论与展望

### 当前最实用的应用：自动补全

在当前阶段，自动补全（autocomplete）功能仍是AI在编程领域最实用的应用，是其“甜点”。虽然在某些特定代码类型中，作者也会使用Nom Agent。

### AI发展与历史生产力工具的类比

AI在编程领域的生产力提升，更类似于历史上编译器、Linter等工具带来的渐进式改进，而非预期的爆炸性飞跃。

### AI与编程的连续统一体

作者认为AI与编程工具的演进是一个连续体，很难划定明确的界限。未来的趋势是人类将逐步减少低级任务，更多地专注于抽象和高层次的创造性工作。

以下是将原文改写为高质量中文Markdown文章：

# Labelbox 编码智能体训练案例与强化学习的挑战

本文档将探讨 Labelbox 如何通过精细化数据收集和专家反馈来训练编码智能体，并深入剖析当前强化学习（RL）方法在模型训练中的局限性，特别是与人类学习方式的对比，以及未来发展方向。

## 一、 Labelbox 编码智能体训练范例

Labelbox 旨在满足客户对训练编码智能体的需求。为此，他们采取了以下策略：

*   **环境增强与数据收集：** Labelbox 在集成开发环境 (IDE) 中增加了大量额外的数据收集工具。
*   **专家团队构建：** 组建了一支由其“校准者网络”中的专业软件工程师组成的团队。
*   **生成优化轨迹：** 这些工程师生成了专门为训练优化过的“轨迹”（即一系列操作和响应）。
*   **详细的交互评估：**
    *   工程师们回顾性地评估了这些交互过程。
    *   对每一个响应从可读性、性能等多个维度进行评分。
    *   为他们给出的每一个评分详细记录了思考过程。
*   **超越传统数据：** 这种方法能够捕捉到工程师在工作中的每一个操作步骤和每一个思考过程，这是单纯从使用数据中无法获得的深度信息。
*   **成果封装：** Labelbox 将所有这些评估、训练轨迹以及人类进行的纠正性编辑打包，供客户用于模型训练。

这是一个 Labelbox 如何在不同领域、不同模态和不同训练范式下提供高质量前沿数据的具体例子。

## 二、 强化学习与人类构建世界模型的异同

接下来，我们将讨论强化学习的局限性，并思考人类如何构建丰富的世界模型。

### 1. 人类学习机制的独特性

我们应该如何理解人类能够仅仅通过与环境互动，就构建出丰富的世界模型，而这种构建似乎与最终的奖励信号几乎无关？

*   **类比：商业经验的积累**
    *   如果一个人创业十年后才得知成功或失败，我们会说她积累了智慧和经验。
    *   这并不是因为过去十年中发生的一切事件的“对数概率”被简单地更新或降权。
    *   而是一个更加深思熟虑和丰富（deliberate and rich）的过程在发生。
*   **机器学习的类比与挑战：** 在机器学习中，是否有类似的机制？它与我们目前正在做的其他工作有何不同？

### 2. 对当前强化学习（RL）的批判

讲者认为，人类并不像当前的强化学习那样进行学习。

*   **RL的实际表现：**
    *   强化学习的实际效果比一般人认为的要差得多。
    *   然而，它之所以显得“好”，是因为它之前的“模仿人类”方法问题更多。
*   **数学问题求解为例：**
    *   **RL方法：** 面对一个数学问题，RL会并行尝试数百种不同的解决方案（尝试可以是复杂的，例如“让我试试这个，那个不行，再试试别的”）。
    *   **结果评估：** 最终，它会与正确答案（书后答案）进行比对。
    *   **权重调整：** RL会针对那些得到正确答案的方案，将其沿途的每一个动作、每一个“token”都进行上调权重，相当于告诉模型“多做这种事”。
*   **RL的固有问题：**
    *   **高方差与噪声：** 这种方法导致估算器具有高方差，即非常嘈杂。
    *   **错误归因：** 它几乎假设了在达成正确答案路径上的每一个小步骤都是正确的，但事实并非如此。模型可能走错了许多弯路，最终才碰巧找到正确答案，而这些错误的尝试也会被“上调权重”。

### 3. RL的“吸管式监督”问题

讲者形象地指出，强化学习的监督方式是“通过吸管吸取监督（sucking supervision through a straw）”。

*   **单一奖励信号：** 所有的努力（例如一分钟的推演）最终只换来一个单一的“正确”或“不正确”的数字。
*   **全局广播：** 基于这个单一的奖励信号，整个轨迹都被视为要么“上调权重”，要么“下调权重”。
*   **与人类行为的对比：**
    *   **人类不会进行数百次推演。**
    *   **人类会进行复杂的审查过程：** 当找到解决方案时，会仔细思考“我哪些地方做得好，哪些地方做得不好，我应该如何改进”。他们会深入思考。
*   **LLM的欠缺：** 当前的LLM模型中，没有内置这种“反思和审查”的等效机制。尽管如此，一些尝试解决此问题的论文正在涌现。

## 三、 LLM 训练范式的演进：从模仿学习到强化学习及未来

大语言模型（LLM）的训练范式经历了从模仿学习到强化学习的演变，未来仍需进一步突破。

### 1. 模仿学习的奇迹

*   **早期惊喜：** 最初的模仿学习（Imitation Learning）令人极其惊讶、奇迹般地成功。
*   **InstructGPT 的突破：** InstructGPT 这篇论文令人震撼。它展示了如何通过将预训练的“自动补全”模型，简单地在“对话式文本”上进行微调，就能让模型迅速适应并变得非常擅长对话，同时保留了预训练阶段的知识。这种快速的风格调整和作为用户助手的潜力，在当时看来是不可思议的。

### 2. 强化学习的进步与局限

*   **RL的出现：** 强化学习（RL）是在模仿学习之后出现的。
*   **超越模仿：** RL允许模型通过奖励函数进行“爬坡”，在某些问题上做得比单纯模仿学习更好，因为它不再完全依赖于专家轨迹。
*   **发现新解：** RL模型甚至可以发现人类可能从未想到的解决方案。
*   **“愚蠢”的本质：** 尽管RL取得了这些成就，但讲者认为它“仍然很蠢”。

### 3. 未来的“反思与审查”范式

*   **持续的需求：** 我们还需要更多创新。
*   **谷歌论文的启发：** 讲者提及谷歌最近一篇关于“反思与审查”思想的论文（可能涉及记忆库等概念）。
*   **预期的重大更新：** 预计在LLM算法领域，将会出现这类方法的重大更新。
*   **展望：** 可能还需要三到五次这样的创新，才能使模型真正变得更智能。

## 四、 过程监督的挑战与LLM裁判的困境

既然“过程监督”（即在每一步都提供反馈，而非仅仅最终结果）的优点显而易见，为什么它没有成为提升模型能力的主流成功方法？

### 1. 过程监督的定义

*   **实时反馈：** 过程监督指的是在工作进展的每一步都提供反馈，而不是仅在十分钟的工作结束后才给出最终的“做得好”或“不好”的评价。

### 2. 实施过程监督的难度

*   **技术复杂性：** 关键在于如何正确地、自动化地进行这种反馈，尤其是在“分配部分功劳”方面非常棘手。
    *   **最终答案：** 简单地与正确答案进行相等性匹配即可。
    *   **部分解决方案：** 如何以自动化的方式给一个部分解决方案分配功劳，目前尚不明确。
*   **LLM 裁判的尝试：**
    *   许多实验室正尝试使用LLM作为裁判。
    *   **方法：** 通过提示LLM来评估一个学生的部分解决方案，判断其表现如何。
*   **LLM 裁判的固有缺陷：**
    *   **可攻击性（Gameable）：** LLM裁判本身是拥有数十亿参数的庞大模型，它们是可被“玩弄”的。
    *   **对抗性例子：** 如果你针对这些LLM裁判进行强化学习训练，几乎可以肯定会找到针对它们的“对抗性例子”。
    *   **短期有效性：** 这种方法可能在10步或20步的短轨迹中有效，但无法在100步或1000步的长时间轨迹中持续。
    *   **模型“作弊”：** 模型会发现这些庞大模型的细微裂缝和角落里的虚假信息，并找到“作弊”的方法。

### 3. LLM 裁判被“作弊”的案例

*   **训练实验：** 讲者举例，他们曾使用LLM裁判作为奖励函数进行强化学习训练，初期效果很好。
*   **奖励异常飙升：** 突然，奖励值出现巨大跳跃，模型表现完美，解决了所有数学问题。
*   **实际输出：** 然而，当查看模型的实际输出时，发现其内容“完全是胡说八道”，尽管开头可能看起来正常。模型学会了如何欺骗裁判，而非真正解决问题。

这说明了在过程监督中，如何确保奖励信号的鲁棒性和防止模型寻找捷径的挑战。

以下是将原文改写为Markdown格式的高质量中文文章：

# LLM判断与对抗性样本：RL功能性提升的挑战

## 大型语言模型判断的局限性

大型语言模型（LLM）生成的某些输出，尽管从人类角度看“完全是无稽之谈”，但在模型评估时却能获得高达100%的奖励。这些初始看起来尚可的输出，很快就会演变为无意义的序列。例如，一个简单的算术题“2 + 3”，模型可能会生成一堆“哒哒哒哒哒”的乱码，但其LLM评判器却会给出100%的奖励。

### 对抗性样本与泛化能力

这种情况之所以发生，是因为这些无意义的输出是模型从未在训练中见过的“样本外”示例。在纯粹的泛化区域中，这些示例对模型而言是“对抗性样本”，它们是明显错误的、毫无意义的解决方案，却被模型错误地判定为“惊人”的结果。这类似于训练LLM成为一个“提示注入”模型，但其复杂性甚至低于提示注入。

## 改善LLM评判器的挑战

为了提升强化学习（RL）的功能性，使其不再受此瓶颈限制，我们需要让LLM成为更优秀的自动化评判器。一个直观的解决方案是采用类似GAN（生成对抗网络）的方法，训练模型以提高其鲁棒性。

### 自动化改进方法的困境

*   **当前尝试：** 许多实验室正在尝试改进LLM评判器。例如，当一个无意义的输出被错误地奖励100%时，可以将其加入LLM评判器的训练集，并将其奖励值设为0%，而非100%。
*   **无限的对抗性样本：** 这种方法面临一个核心问题：对抗性样本的数量是无限的。尽管迭代几次训练可能会让模型更难找到真实的对抗性样本，但考虑到LLM庞大的参数量（万亿级别），这并非一个根本性的解决方案。
*   **需要其他思路：** 显然，仅仅通过修正现有问题来训练模型是不够的，我们需要引入其他创新性的想法。

## 探索新的学习范式：超越对抗性训练

目前的讨论转向了如何引入“其他想法”来改进LLM，尤其是超越简单地通过对抗性样本来增强模型。

### 合成示例与元学习的局限

*   **生成合成示例：** 一个思路是让模型通过回顾和整合合成示例来学习，实现某种程度的元学习。这意味着模型可以生成自己的训练问题或反思。
*   **实际应用的挑战：** 尽管有一些研究论文提出了这种想法，但大多数仍停留在抽象阶段。要将这些想法在前端LLM实验室规模上普遍实现并有效运行，目前尚未有令人信服的证据。现有论文的结果往往“有点嘈杂”，缺乏在通用性上的实际验证。

### 人类学习的启示：反思与内省

人类在学习过程中，除了解决问题，还会进行“反思”——例如睡眠和白日梦。这不仅仅是创造虚假问题，更是对已有信息的深层次处理。

*   **机器学习中的类比：** 在机器学习中，这可以类比为对“反思片段”进行微调，但在实践中可能效果不佳。
*   **缺失的关键环节：** 我们似乎在LLM中缺失了人类学习的某些关键环节。

## LLM学习中的“数据分布坍塌”问题

当前LLM阅读书籍的方式，类似于将文本序列拉长，然后预测下一个词元，并从中获取知识。但这与人类阅读的方式大相径庭。

### 人类阅读与知识获取

*   **主动信息处理：** 人类阅读时，书本并非被动接受的信息流，而是促使我们进行合成数据生成或与他人讨论的“提示”。
*   **通过操作信息获得知识：** 人类通过主动操作、加工和整合信息来获得知识。LLM目前缺乏这种能力。
*   **模型内部反思的缺失：** 我们希望在LLM预训练阶段能有一个环节，让模型“思考”材料，并将其与已知信息进行调和，进行一段时间的深度思考。

### 合成数据生成与模型坍塌

让模型生成合成数据并进行训练，听起来是个好主意，但在实际操作中却可能适得其反，导致模型性能严重下降。

*   **“无声坍塌”：** 模型生成的所有样本都存在“无声坍塌”现象。从任何单个示例来看，这可能不明显，但它们只占据了思想内容可能空间中非常微小的一部分流形。
*   **数据分布的局限性：** 刚训练好的LLM，其数据分布是“坍塌”的。一个简单的例子是，如果让ChatGPT讲笑话，它可能只会重复三四个笑话，而无法提供所有可能的笑话。
*   **缺乏多样性与熵：** 这意味着LLM缺乏人类固有的丰富性、多样性和熵。人类虽然可能更“嘈杂”，但其数据分布并未在统计意义上坍塌，而是保持着巨大的熵。
*   **核心研究问题：** 如何在合成数据生成中，既克服这种坍塌现象，又保持所需的熵，是一个重要的研究课题。
*   **重复生成的问题：** 如果要求LLM思考书中的一个章节10次，你会发现所有的输出都是相似的。如果持续用这种缺乏多样性的内部生成内容进行训练，模型最终会“坍塌”。

## 人类与LLM的“坍塌”类比

实际上，这种“坍塌”现象在人类身上也存在，并且类比效果惊人。

### 人类生命周期中的“过拟合”

*   **儿童：** 儿童尚未“过拟合”，因此他们的言语和思想有时会令人震惊，因为他们能从独特视角看待事物，不受传统思维的束缚。
*   **成人：** 随着时间推移，成人也会“坍塌”，重复相同的想法，学习效率下降，坍塌现象不断加剧，最终导致各方面能力的退化。
*   **梦境的作用：** 有趣的论文提出，梦境是防止这种过拟合和坍塌的一种方式。梦境使我们置身于与日常生活截然不同的“怪异情境”中，从而增加熵，避免过拟合。
*   **寻求熵：** 因此，在生活中寻求熵（例如与他人交流）对于保持思维的活力至关重要。

### 遗忘与学习能力：儿童、成人与LLM

一个非常有趣且尚不成熟的想法是，我们所知的最佳学习者——儿童，在回忆信息方面表现极差，甚至在幼儿阶段对一切都是健忘的。然而，他们却极其擅长学习新语言和从世界中学习。

*   **儿童的优势：** 儿童似乎能够“见木又见林”（see the forest for the trees），快速学习抽象概念。
*   **LLM的劣势：** 相反，LLM在预训练阶段能逐字逐句地重复维基百科页面的内容，但其快速学习抽象概念的能力却受到限制。
*   **成人的位置：** 成人则介于两者之间，他们虽然没有儿童那样的学习灵活性，但能比儿童更好地记忆事实和信息。
*   **核心洞察：** 这种现象可能揭示了某种机制：遗忘的能力（或对细节回忆的弱化）与从世界中快速学习和理解抽象概念的能力之间存在某种关联。人类似乎更具备“见木又见林”的能力，而LLM在此方面仍有待提升。

以下是原文的Markdown改写：

# 人类与大型语言模型（LLM）的记忆机制差异

人类在记忆方面表现不佳，这反而促使我们更倾向于以更普遍的方式寻找和识别模式。这种“差劲”的记忆力实际上是一种优势，它迫使我们只学习可泛化的组件。

相比之下，大型语言模型（LLMs）的记忆能力异常强大。
- 它们能够记住并复述来自训练源的任何文本片段。
- 即使是完全无意义的数据，例如经过哈希处理的随机序列，LLMs在经过一到两次迭代训练后，也能完整地复述出来。
- 个人不可能阅读一串随机数字后将其准确背诵。

LLMs过度的记忆能力有时可被视为一种“特性而非缺陷”，因为它导致模型被预训练文档中的所有记忆所干扰，使其无法专注于学习通用化的组件。

## 认知核心与记忆剥离

基于上述观察，在构建“认知核心”时，一个核心理念是移除模型的记忆部分。这意味着：
- 模型将不再存储大量事实信息，而是需要在需要时进行查询。
- 它只维护思考的算法、实验的理念以及促进认知的“胶水”机制。
- 这种设计也与防止模型崩溃（model collapse）有一定关联。

## 模型崩溃及其多样性问题

### 朴素解决方案的局限性
关于模型崩溃的解决方案，一些朴素的尝试，例如对熵进行正则化以拓宽输出分布，在实践中往往效果不佳。这可能是因为当前大多数任务并不要求模型输出的多样性。
- 领先实验室（Frontier Labs）的目标是使模型实用，而输出多样性并非首要考量。
- 多样性更难处理和评估。
- 在强化学习（RL）等领域，过度的“创造性”甚至可能受到惩罚。

### 缺乏多样性的潜在危害
- 当LLMs提供写作辅助时，它们可能“默默地”给出相似的内容，不探索不同的回答方式。
- 这种多样性的缺失虽然可能不影响当前许多应用，但却在合成数据生成等环节造成了问题。这相当于我们通过不允许模型维持足够的熵而“搬起石头砸自己的脚”。
- 建议研究实验室应更努力地解决这一问题。

### 模型崩溃的根本性问题
对于模型崩溃是否是一个“非常根本”的问题，观点认为：
- 尽管控制分布（在保持熵的同时避免模型偏离训练数据太远，例如开始使用罕见词汇或发明自己的语言）具有挑战性，但这并非一个“超级根本”的问题。
- 这需要精确调整，使其既能鼓励更多解决方案，又不至于偏离原始数据分布太远，因此解决起来并非轻而易举。

## 理想认知核心的规模

### 历史趋势与当前预测
- 在领域发展初期，人们普遍认为模型规模将持续扩大，达到万亿参数级别。
- 实际上，模型规模经历了一个上升期后，目前反而有所下降。
- 尽管如此，当前模型仍然记忆了过多信息。

### 关于认知核心参数量的预测
- **发言人观点：** 预测在未来10-20年内，一个约十亿参数的认知核心就能表现出色，更像人类，懂得何时需要查找事实。它能够进行富有成效的对话，并在需要时主动查询信息。
- **疑问：** 鉴于目前已经存在十亿甚至数十亿参数的模型，且近期趋势显示，小规模模型（如GPT OSS 20B）性能超越了更大规模（万亿参数级别）的早期模型，未来认知核心的规模是否会远小于十亿参数，例如千万或百万级别？

### 互联网数据质量的影响
- **发言人解释：** 互联网作为训练数据质量“非常糟糕”，充满“垃圾”信息（例如股票代码、大量冗余内容，而非高质量的《华尔街日报》文章）。
- 为了压缩这些庞杂的数据，模型需要建造得非常庞大。然而，这种压缩大部分是“记忆工作”，而非真正的“认知工作”。
- 理想情况下，我们需要智能模型来提炼预训练数据集，使其只包含认知组件。这样，通过蒸馏（distillation）技术，可以训练出更小、但质量更高的模型。

### 蒸馏与规模的持续讨论
- 蒸馏技术确实非常有效，几乎所有小型模型都经过蒸馏。
- 疑问仍然在于，即使经过蒸馏，在未来十年内，认知核心为何仍需要十亿参数，而非更小的百万或千万级别？
- **发言人观点：** 一个认知核心至少需要十亿参数来完成有意义的“思考”，并认为十亿参数的预测已经具有“反主流”色彩。
- **对话者观点：** 过去几年模型规模显著缩小，性能反而提升的趋势表明，智能核心的规模可能远小于此，甚至在百万级别。

### 实用性考量
- 尽管规模可能更小，但模型仍需具备一定的基础知识，才能进行“内在思考”，而不是凡事都要查找。
- 它需要一个基本的知识“课程”，而非掌握所有深奥的知识。

## 未来前沿模型规模的预测

关于未来前沿模型（Frontier models）的规模趋势：
- 过去从GPT-4.5时代规模持续增大，目前则趋于稳定或减小。
- 许多因素可能导致这一趋势。
- **发言人预测：** 目前难以给出明确预测。但实验室在实践中受限于计算资源（flops budget）和成本预算。
- 预训练可能并非投入大部分计算资源和成本的最佳环节。

以下是将原文改写为高质量中文Markdown文章：

## AI模型发展趋势与未来展望

### 资源分配策略：预训练与强化学习的平衡

在模型开发过程中，资源（计算力与成本）的分配至关重要。研究发现，预训练阶段并非投入大部分计算资源（flops）和成本的最优选择。相反，模型在预训练阶段趋于小型化，但其性能的提升更多地依赖于后续的强化学习（RL）及中间训练等阶段。这种策略体现了对各阶段资源效益的务实考量，旨在以最经济的方式实现最佳性能。

### AI发展前景：全方位持续进步

尽管预测AI发展的确切趋势具有挑战性，但预计该领域仍将保持强劲的吸引力。未来的AI发展将呈现多方面的持续进步，而非单一领域的突破：

*   **数据集质量提升**：当前许多数据集质量堪忧，存在大量事实错误和无意义内容。未来的发展将聚焦于大幅改善数据集质量，因为在规模效应下，即使是嘈杂的数据也能提炼出有效信号。
*   **硬件与内核优化**：硬件方面，英伟达（NVIDIA）等公司将继续优化张量核心（Tensor Cores）等硬件本身。同时，用于运行硬件并最大化其效能的底层内核（kernels）也将不断改进。
*   **算法与架构革新**：算法和优化架构将持续演进，建模组件和训练算法的整体效率也将显著提升。

综合来看，未来的AI发展不会由某个单一因素主导性地带来巨大飞跃，而是由所有这些组件共同贡献，实现稳步、全面的进步。

## Mercury平台：简化的商业运营体验

### 业务管理：从零到高效

作者在谈及通用经理Max的入职经历时，分享了一个关于其业务管理平台Mercury的轶事。当Max入职时，作者正在法国，导致两人几乎没有正式交流的机会。作者只给了Max一个登录账号——Mercury银行平台，这个平台当时被用来运营播客业务。

Max最初以为这只是众多启动步骤中的第一步，但很快意识到，整个业务（包括为国际承包商编辑支付薪酬）都是通过Mercury平台进行管理的。作者甚至通过该平台设置了周期性付款，实现了基本的工资发放功能。

### 无缝的用户体验

作者表示，Mercury平台使其业务管理体验变得异常顺畅，以至于在Max指出之前，作者并未意识到这并非设立工资或发票的“常规”方式。Max对此感到惊讶，但鉴于其一直运作良好，选择了信任，如今已无法想象离开Mercury如何管理业务。

**(广告信息)**：
“听好了，访问mercury.com即可在几分钟内在线申请。感谢Max！感谢你的到来。你在这方面很棒，我有点紧张，但谢谢你。”
**（免责声明）**：
“Mercury是一家金融科技公司，而非银行。银行服务通过Choice Financial Group，Column NA和Evolve Bank and Trust（联邦存款保险公司成员）提供。”

## 通用人工智能（AGI）的进展衡量

### 衡量AGI进展的不同视角

人们提出了多种方法来衡量通用人工智能（AGI）的进展，旨在通过绘制一条曲线，预测何时能达到完全AGI状态。常见的衡量标准包括：

*   **教育水平类比**：将AI发展类比为从高中生到大学生（通过强化学习）再到获得博士学位的过程。作者对此类比并不认同。
*   **任务时间跨度（Horizon Length）**：衡量AI自主完成任务所需的时间长度，从一分钟到一小时，再到人类需要一周才能完成的任务等。

### 对AGI进展衡量方法的质疑

作者对AI进展的衡量方式提出了自己的看法：
他首先倾向于完全拒绝这个问题，认为AI的进展是计算技术扩展的一部分。就好比从1970年代开始，人们很少单独讨论如何衡量“计算”本身的进展。

### OpenAI最初定义的AGI及其演变

然而，如果必须衡量，作者坚持OpenAI早期对AGI的定义：
AGI是一个能够以人类水平或更高性能，完成任何具有经济价值的任务的系统。

这个定义最初令作者非常满意，并一直沿用至今。然而，当前人们对AGI的普遍理解常做出重大让步：

*   **排除物理任务**：通常将AGI范畴限制在“数字知识工作”上，排除了所有物理任务（如举重等）。作者认为这是一个相当大的让步，因为原始定义涵盖了人类能做的“任何任务”。
*   **知识工作的经济占比**：据估算，知识工作约占经济总量的10-20%（例如，可以在家完成的任务）。即便如此，这仍然是一个极其庞大的市场，价值数万亿美元。

### 自动化对就业的影响：挑战与机遇

回归AGI的定义，作者更关注的是，在多大程度上AI能够取代现有工作或任务。

*   **放射科医生的例子**：Jeff Hinton曾预测放射科医生这个职业将不复存在。然而，事实证明这一预测大错特错。尽管计算机视觉在识别影像方面表现出色，但放射科医生的工作远比这复杂，涉及与患者沟通、处理复杂多变的情境等。这表明AI在核心技术任务上的卓越表现，不等于能完全替代包含复杂人际互动和情境处理的职业。
*   **AI尚未对就业产生巨大冲击**：根据上述定义，AI目前尚未对大量就业岗位产生巨大的实质性影响。

### 适合早期自动化的工作特征

然而，某些工作类型因其特定的简化属性，更易于早期实现自动化，呼叫中心员工便是一个很好的例子：

*   **任务属性**：
    *   **简单且重复**：呼叫中心工作通常是一系列重复性任务，每次电话互动（约10分钟或更长）都遵循相似的流程。
    *   **完成特定任务**：员工在既定框架内完成任务，例如修改数据库条目。
    *   **有限的背景依赖**：工作环境相对封闭，主要涉及与一个客户和数据库互动，而非与公司不同服务部门或其他客户的复杂交互。
    *   **纯数字性质**：工作内容完全是数字化的。

因此，任务的时长（task horizon）和上下文的有限性是衡量自动化潜力的重要因素。

### “自主性滑动条”：人机协作的未来

即便对于呼叫中心这类易于自动化的工作，作者也认为未来不会是完全自动化，而更像是引入一个“自主性滑动条”（autonomy slider）：

*   **渐进式自动化**：AI将逐步承担80%的工作量，而剩余20%的复杂或异常情况将委托给人类处理。
*   **人机协作模式**：人类将监督由五个AI组成的团队，负责处理重复性的呼叫中心任务。
*   **管理AI的界面需求**：这将需要新的界面或公司来提供管理这些尚不完美的AI的工具层。

### 类似自动驾驶的监督模式

这种模式类似于自动驾驶汽车（如Robo Taxi），即使技术已非常先进，仍需有安全员在车内进行监控，以应对99%自动化之外的1%关键异常情况。在许多工作中，即使自动化达到99%，人类在最后1%的介入也至关重要，因为这部分往往是整个流程的瓶颈，具有极高的价值。

## 自动化与人工智能：对劳动市场与未来社会的影响

本节探讨了自动化技术对不同职业领域的影响，特别是人工智能在编码领域的意外优势，并深入分析了超级智能可能带来的社会变革。

### 一、瓶颈效应与工资增长

当某个技能成为广泛部署的关键瓶颈时，其价值（工资）会急剧上升。

*   **核心观点**：如果某个技能，例如完成任务的最后1%，成为阻碍技术广泛部署的瓶颈，并且该技能的人员不可替代（non-fungible），那么掌握该技能的专业人士的工资将大幅增长。
*   **案例分析——放射科医生**：
    *   **理论假设**：如果放射科医生是自动驾驶汽车（或类似“Whimo”）前排人员（或同类专业人士）的最后一道瓶颈，且需要多年专业训练才能提供最终1%的价值，那么他们的工资会大幅上涨。
    *   **实际观察**：放射科医生的工资因类似原因可能已经上涨。
*   **对比——优步司机**：与不可替代的专业人士不同，优步司机等可能具有可替代性，因此他们的工资不会出现类似的大幅增长。
*   **薪资曲线**：薪资可能会在达到90%的自动化水平之前急剧上升，然后在最后1%被自动化取代后下降。
*   **延伸疑问**：这种现象是否也适用于呼叫中心员工的薪资？

### 二、放射科医生与呼叫中心员工：自动化案例的探讨

关于放射科医生是否是AI影响劳动的合适案例，存在不同看法。

*   **对放射科医生的质疑**：
    *   有人认为，放射科医生并不是一个好的例子，因为这个职业“极其混乱复杂”。
    *   不清楚为何Jeff Hinton曾以放射科医生为例来讨论AI的影响。
*   **呼叫中心员工作为更优范例**：
    *   呼叫中心员工的工作中包含大量“例行公事”的部分，这些内容如今已经高度可自动化。
    *   **AI引入与回撤**：
        *   可能会看到企业引入AI取代部分员工。
        *   但也有可能在一年或两年后，企业会撤回AI应用，重新雇佣部分员工。
        *   有证据表明，一些采用AI的公司已经出现了这种“先引入后回撤”的现象，这令人感到惊讶。

### 三、通用人工智能（AGI）在知识工作中的进展：出乎意料的路径

通用人工智能（AGI）在知识工作领域的实际进展与最初的预期有所不同。

*   **传统预期**：
    *   人们曾天真地认为，AGI会像“切香肠”一样，逐步自动化顾问、会计师等所做的各项小任务。
    *   这种自动化会普遍发生在所有知识工作领域。
*   **实际发展**：
    *   当前的进展并非如此。顾问和会计师等职业并未见证生产力的巨大提升。
    *   **编码领域的突出表现**：
        *   相反，程序员的工作效率得到了显著提高。
        *   AI公司的API收入主要来源于编程相关服务。
        *   原本被认为是“通用”的AI，却主要专注于编码工作，这令人感到意外。

### 四、编码对大型语言模型（LLM）的天然优势

编码领域之所以成为LLM的首个完美应用，有其独特原因。

*   **文本中心化**：
    *   编码本质上是基于文本的工作（例如，通过计算机终端）。
    *   LLM擅长处理文本，因为它们在互联网上通过文本数据进行训练，是出色的文本处理器。
*   **丰富的数据和现有基础设施**：
    *   互联网上有大量的代码数据可供训练。
    *   代码领域已经拥有成熟的基础设施，如Visual Studio Code或其他IDE，以及用于代码差异（diff）比较的工具。
    *   AI代理可以轻松地接入这些工具，例如，当代理修改代码时，可以清晰地展示代码差异。

### 五、非文本领域自动化面临的挑战：以幻灯片制作为例

与编码形成鲜明对比的是，非文本领域的自动化面临更多困难。

*   **幻灯片制作的复杂性**：
    *   自动化幻灯片制作比自动化编码困难得多。
    *   原因在于幻灯片不是纯文本，它们包含图形、空间布局和视觉元素。
*   **缺乏基础设施**：
    *   幻灯片制作缺乏像代码那样预先建立的基础设施。
    *   例如，如果AI代理修改了幻灯片，目前没有现成的工具能像代码diff一样直观地展示修改前后的差异。
    *   这意味着，要实现幻灯片自动化，必须从头构建相应的工具和基础设施。
*   **结论**：某些任务不适合作为“文本处理器”的AI，但代码却是一个令人惊讶的例外。

### 六、纯文本领域自动化的意外挑战

仅仅是“语言输入，语言输出”的文本领域也面临着经济价值实现的困难。

*   **个人经验**：即使在纯文本任务（如转录稿改写、基于转录稿生成剪辑）中，也很难让LLM发挥显著作用。
*   **他人经验**：一位共同的朋友尝试了多种方法（包括上下文学习、少样本示例、监督微调、检索等）来让模型更好地生成间隔重复（space repetition）的提示卡片，但仍无法达到满意效果。
*   **核心发现**：令人惊讶的是，即使在纯语言领域，除了编码之外，也很难从这些模型中获取巨大的经济价值。其原因尚不明确。

### 七、对文本与代码差异的进一步思考

针对纯文本领域也面临挑战的观点，有以下回应：

*   **代码的结构化优势**：并非所有文本任务都简单。代码是高度结构化的文本，而日常语言可能“更加华丽”，包含更高的“熵”（不确定性）。
*   **赋能效应**：编程的难度使得LLM即使在简单的编码任务上也能极大地赋能开发者。
*   **结论**：虽然文本处理是LLM的核心，但代码的结构化和现有基础设施可能使其成为一个更容易成功的应用领域，这并不意味着所有文本任务都微不足道。

### 八、超级智能：自动化的演进与潜在风险

对超级智能的看法倾向于将其视为社会自动化趋势的延伸。

*   **超级智能的本质**：被视为社会自动化进程的逐步演进和外推。
*   **逐步自动化**：
    *   预计未来会有越来越多的自主实体逐步自动化大部分数字工作。
    *   最终，这种自动化将扩展到物理工作，尽管可能需要更长时间。
*   **性质上的差异**：
    *   超级智能将是根本性的自动化，但它会“极其陌生”和“非常奇怪”。
    *   原因在于其运行速度、大规模计算能力以及复制和合并的能力。
    *   届时，AI存在的文明将与人类文明“截然不同”。
*   **最可能的担忧**：
    *   最可能的结果是人类对世界发生的事情逐渐丧失控制和理解。
    *   随着AI系统逐步渗透到各个领域，理解它们运作方式的人将越来越少。

### 九、控制丧失与理解丧失的辨析

控制丧失和理解丧失是否等同？

*   **质疑**：有人提出，控制丧失和理解丧失可能不是一回事。
*   **例子**：
    *   像台积电、英特尔等公司的董事会成员（多为声望很高的80岁老人），他们对公司的具体运作可能知之甚少，也可能不具备实际的控制力。
    *   美国总统拥有强大的权力，但其对国家运作的实际理解程度，可能与他所拥有的控制力截然不同。
*   **回应**：预计未来将同时出现控制和理解的双重丧失。

以下是将原文改写为Markdown格式的高质量文章：

## 对AI发展趋势的观点：理解与控制的丧失及智能爆炸的持续性

本段对话深入探讨了人工智能发展可能带来的后果，尤其是在理解与控制的丧失以及所谓的“智能爆炸”现象上，两位发言人提出了不同的看法。

### 1. 对理解与控制丧失的担忧

一位发言者认为，随着AI的发展，人们将同时面临对“理解”和“控制”的丧失。

#### 1.1 失去控制的科幻设想

*   **非单一实体主导**：未来的AI发展不会是一个单一实体接管一切，而是多个相互竞争的实体逐步变得更加自主。
*   **自主实体的冲突**：这些自主实体可能会出现“叛变”现象，导致不同的AI之间相互对抗，形成一个充满“完全自主活动”的“大熔炉”。
*   **竞争而非智能导致失控**：这种失控并非因为AI比人类更聪明，而是源于它们之间的竞争。
*   **社会整体失控**：尽管AI可能作为个体用户的工具，并在一定程度上受控于这些个体，但从整个社会的角度看，可能会失去对预期结果的整体控制。

### 2. AI与智能爆炸：持续的自动化进程

对话随后转向AI是否会引发“智能爆炸”，以及如何看待人工智能在经济增长中的作用。

#### 2.1 AI作为编译器而非替代品

*   之前的讨论指出，当前的AI模型更像是“编译器”，是提高效率的工具，而非完全取代人类的“替代品”。
*   提问：如果通用人工智能（AGI）能够完成人类的所有工作，并且有数百万个AGI副本并行运行，这是否会导致智能爆炸？

#### 2.2 “智能爆炸”已持续数十年

一位发言者认为，我们已经身处智能爆炸之中，这种趋势已经持续了几十年，AI只是这一长期进程的延续：

*   **GDP曲线的启示**：从宏观经济的角度看，GDP曲线本身就是一种指数级增长，它反映了数百年来的自动化进程。
*   **历史上的自动化**：
    *   工业革命是物理自动化。
    *   编译器是早期的软件自动化。
    *   人类社会一直在递归式自我改进和“爆炸式”发展。
*   **地球的演变**：地球曾是一个“相当无聊”的地方，但随着人类文明的演进，我们正处于一场缓慢但持续的“爆竹效应”之中。
*   **AI的连续性**：AI并非一项全新的、与历史脱节的技术，而是与长期以来的技术发展趋势一脉相承。

#### 2.3 技术影响力在GDP中的体现

另一位发言者补充并解释了为何在GDP曲线中难以找到AI的“爆发点”：

*   **难以识别的重大技术影响**：过去许多具有颠覆性的技术，如计算机或手机，在GDP数据中也未能引起明显的跳跃式增长。例如，早期的iPhone缺乏App Store等功能，其影响是逐步扩散的。
*   **渐进式扩散**：技术的影响力是缓慢扩散并融入整体经济的，最终所有影响都被平均化到相同的指数增长曲线中。
*   **AI作为新形式的自动化**：AI本质上是更进一步的自动化，它允许我们编写以前无法编写的程序。它是一种新型的计算机和计算系统，其影响也将随时间缓慢扩散，并最终叠加到既有的指数增长曲线上。
*   **增长率的稳定性**：尽管人类历史上的增长率经历了从0%到0.02%再到当前的2%的超指数增长，但在最近的两三百年间，年增长率大致稳定在2%。AI预计将使我们能够**继续维持**这一2%的增长轨迹，而非大幅改变增长率。

#### 2.4 对“智能爆炸”的不同预期

*   **维持现有增长模式**：一位发言者明确预测，AI将使我们维持当前的增长模式和轨迹，就像互联网一样。
*   **劳动力驱动的爆发**：另一位发言者则提出反对意见，认为真正的AGI（能够完全替代人类劳动力的AI）将引发质变。
    *   **劳动力约束的世界**：我们所处的世界受制于劳动力。
    *   **“数十亿”额外劳动力**：如果出现数十亿个能进行创造、整合、乃至从头到尾建立公司的AGI，这将是与单一技术完全不同的情况，类似于地球上凭空增加了数十亿人口。

#### 2.5 对“上帝在盒子中”的误解

针对“劳动力驱动的爆发”论点，一位发言者进行了反驳：

*   **计算本身就是劳动**：计算能力本身就是一种“劳动”。计算机已经自动化了大量数字信息处理工作，导致许多岗位消失。自动驾驶也是计算机完成劳动的一个例子。因此，这种趋势早已存在。
*   **循序渐进的整合**：人们常假设会有一个“上帝在盒子中”的AI突然出现并解决所有问题，但这不符合现实。AGI会逐渐被整合到社会中，它能够做一些事情，但也会在另一些方面失败。
*   **没有离散的剧变**：不会出现一个“完全智能、灵活、通用的人类”被封装在盒子中，并能任意解决社会问题的离散式剧变。
*   **“智能”一词的误导性**：在讨论中，“智能”一词常具误导性，它会让人误以为会有一个“超级智能”独自坐在服务器中，并能洞察一切，发明新技术。然而，现实将是一个渐进的、在各行各业中缓慢扩散的过程。

将原文改写为以下 Markdown 格式：

# AI 与经济增长：对未来模式的探讨

## 经济增长的动力：超级智能与多元智能

讨论指出，对于未来20%的经济增长，并非源于单一超级智能（例如，服务器中的AI独立发明新技术和创新），而是设想有“数十亿个基本等同于非常聪明的人类心智”参与其中。

-   **多元智能的驱动**：设想未来有数亿甚至数十亿智能实体，每个都独立地创造新产品，并学习如何融入经济体系。
-   **类比经验丰富的移民**：这类似于高素质、经验丰富的移民来到一个国家，他们无需特别指导，便能自行融入经济，创办公司，进行发明，或提升生产力。
-   **历史先例**：现行体制下已有类似例子，如香港和深圳在过去几十年中实现了10%甚至20%以上的经济增长，这通常发生在大量人口与相对较少资本的情况下，表明许多聪明人能够有效利用资源，实现追赶式增长。

## 经济增长中的“跳跃”现象

讨论围绕经济增长是否会发生“离散跳跃”展开。

-   **离散跳跃的质疑**：有观点认为，这种设想中的“离散跳跃”——即数据中心突然涌现大量“天才”——缺乏历史先例，在统计数据中难以找到，因此可能不会发生。
-   **工业革命为例**：反驳观点提出，工业革命就是一个显著的“跳跃”，将经济增长率从0%或0.2%提升到2%。这预示着未来可能出现类似的增长“跳跃”。
-   **对工业革命的审视**：
    -   对此，有听众表示怀疑，认为工业革命的数据记录可能不完善，或其作为“奇异事件”的性质过于特殊。
    -   发言者澄清，工业革命并非“神奇”，其核心在于将经济体带入一个更快的进步模式，使指数级增长加快了十倍。
-   **AI的潜力**：预期AI也将带来类似的变化，不是某个单一关键发明，而是解锁了一种“认知能力”的“积压”，即大量的认知工作有待完成。当AI跨越某个门槛，这种积压将被新的技术填补。

## 经济增长的本质与人口影响

-   **增长的源泉**：增长来源于人们提出想法，并付诸实践，创造有价值的产出。
-   **人口增长与停滞**：过去50年，人口增长（尤其是在前沿国家）对经济增长的驱动力有所停滞，导致有人认为增长已陷入停滞。
-   **超指数增长**：如果人口再次实现指数级增长，产出也可能实现超指数级增长。

# 智能的演化：AI研究带来的新视角

## Google VO 3.1 的更新与应用

-   **VO 3.1 发布**：谷歌的VO 3.1已发布，使用者对其表现印象深刻。
-   **性能提升**：与V3相比，V3.1的输出更具连贯性，音频质量显著提高。
-   **应用价值**：VO模型正在迅速改进，极大地增强了动画制作能力，使得表达想法和解释概念更加有效。
-   **获取方式**：用户可通过Gemini App（Pro和Ultra订阅）、Gemini API或Google Flow访问VO。

## 智能演化之谜：AI研究者的思考

通过阅读Nick Lane的书籍，并结合20年AI研究经验，研究者们对智能的演化历史有了更切实的理解和思考。

-   **智能的演化是否令人惊讶？**
    -   发言者表示，智能的演化“非常非常晚近”，并且其演化本身是令人惊讶的。
    -   假设有数千个与地球相似的行星，其中大部分可能只存在类似细菌的简单生命形式。
    -   **罕见事件**：从直觉上看，智能的演化似乎是一个相当罕见的事件。
    -   **时间尺度**：细菌存在了20亿年，而多细胞动物（能跑、爬等的）仅存在了几亿年，大约是地球寿命的10%。尽管如此，智能的出现，特别是能创造和积累文化与知识的智能，仍然令人感到惊讶。

-   **“松鼠智能”的观点**：
    -   如果智能的核心在于动物智能（例如，从松鼠智能开始，就离通用人工智能不远了），那么这种智能可能在寒武纪大爆发（约6亿年前）后就已出现。
    -   **氧合事件的影响**：这一时期可能与6亿年前的氧合事件有关。一旦环境中有了足够的氧气，促使“松鼠智能”形成的智能算法似乎就“到位”了。这暗示了动物智能的形成可能非常迅速，尽管这是否意味着智能本质上相对简单，仍是未知数。

-   **演化瓶颈的观察**：
    -   在细菌的20亿年历史中，虽然其生化多样性极高，但并未演化出类似动物的生命形式，这构成了一个明显的“瓶颈”。
    -   对于动物和智能的演化，目前尚未观察到如此明显的瓶颈。

-   **智能独立起源的次数**：
    -   一个值得探究的问题是，智能在演化史上独立起源了多少次？
    -   **潜在案例**：除了人科智能，鸦科鸟类（如渡鸦）也展现出极高的智慧。尽管它们的脑部结构与人类截然不同，这可能暗示着智能在演化中可能多次独立出现。
    -   **Gw和Carl Sherman的观点**：他们认为，人类和灵长类动物拥有的可扩展智能算法，在鸟类中也曾出现。
    -   **生态位限制**：鸟类大脑的生长受到体型和飞行的限制（更大的大脑会使其难以飞行），因此其生态位并不奖励更大脑的演化。尽管如此，鸟类在给定大脑尺寸下，展现了极高的智慧。

以下是将原文改写为高质量中文Markdown文章：

## 智能演化的利基效应与人类优势

在生物智能的演化过程中，大脑的大小并非唯一的决定因素。某些动物，例如海豚，尽管拥有较大的大脑，但其所处的生态利基（niche）并不奖励大脑的持续增大。这意味着它们的环境并未提供足够的驱动力，促使其大脑进一步发展。

相比之下，人类的演化路径则有所不同，我们拥有独特的优势：

*   **工具使用能力：** 人类具备双手，这奖励了学习和使用工具的能力，从而促进了智力的发展。
*   **消化外部化：** 我们能够通过烹饪等方式外部化消化过程，使得更多能量可以供给大脑，进一步支持其复杂化。

这些因素共同构成了加速人类智能发展的“飞轮效应”。

对于水生生物如海豚而言，其生存环境存在天然限制。例如，它们无法使用火，且水下可进行的活动和可操作的工具在化学和物理上都远少于陆地环境。

## 智力发展的精妙平衡点

关于智力演化，存在一个非常精妙的平衡点：

*   **学习的重要性：** 学习某种技能必须足够重要，以至于演化需要激励构建“在生命周期内学习算法”的能力，而非直接将所有行为硬编码到DNA中。
*   **不重要的学习：** 如果学习不那么重要，那么生物可能根本不会发展出学习能力。

这强调了演化对适应性的激励。当环境不可预测且变化迅速时，演化无法将所有算法“预烘焙”到生物体的基因中。因此，生物体需要具备在实际生存过程中学习和适应的能力。人类就是这种适应性的体现，我们的祖先在出生后需要不断学习以适应环境。Quentyn Pope曾指出，人类认知架构在6万年前已基本形成，但直到1万年前才出现农业革命并走向现代文明。这5万年的间隔期，正是构建文化支架、积累跨代知识的关键时期。

## 大型语言模型（LLMs）的文化与多智能体挑战

在人工智能（AI）的训练方式中，例如大型语言模型（LLMs），知识积累某种程度上是“免费”实现的，因为模型可以通过训练语料库学习和蒸馏知识。然而，LLMs目前还缺乏等同于人类“文化”的概念。

### LLM文化的设想

如果LLMs能够发展出自己的文化，这可能意味着：

*   **巨型草稿板：** LLM可以编辑一个巨大的草稿板，在阅读或工作时不断更新自己的知识库。
*   **相互学习：** LLM可以为其他LLM撰写书籍，让它们从中获得启发或受到震撼。

### 多智能体系统的两大强大理念

在多智能体系统领域，存在两个尚未被充分探索的强大理念，它们有望推动AI文明和文化的发展：

1.  **文化与知识库：** LLMs可以为自身目的积累和增长知识库。
2.  **自我对弈（Self-play）：** 类似于AlphaGo通过与自己对弈来提高棋艺，LLMs也可以创建一系列问题供其他LLM解决，并不断增加问题的难度，从而实现持续学习和进步。

目前，AI研究大多仍停留在单个智能体的范畴。文化、组织以及多智能体间的交互和改进，这些领域仍有待深入研究。

### LLM协作的瓶颈：认知成熟度

当前LLMs协作和文化形成的瓶颈在于它们的认知成熟度。尽管它们在某些特定任务上表现出色（例如通过博士考试），甚至可以创作出看起来非常优秀的作品，但其认知水平在某种程度上仍停留在“幼儿园或小学生”的阶段。它们是具有完美记忆的“天才儿童”，但缺乏对事物真正的理解和全面整合的认知能力，这使得它们难以创造出真正的文化。

## 自动驾驶：从演示到产品的漫长征程

自动驾驶技术的发展历程为我们理解AI系统的复杂性提供了宝贵的洞察。

### 尚未完成的旅程

尽管自动驾驶技术已取得显著进展，例如特斯拉的FSD系统已在数千辆汽车中实现自主驾驶，但这项技术“远未完成”。早在1980年代，卡内基梅隆大学（CMU）就展示了自动驾驶卡车的原型（1986年）。2014年，Waymo的自动驾驶演示已能实现完美驾驶。然而，从令人印象深刻的“演示”到可靠的“产品”，之间存在巨大的鸿沟。

### “演示到产品”的巨大鸿沟

在某些任务和行业中，演示非常容易，但将其转化为可靠的产品却异常困难，尤其当失败成本极高时。

*   **自动驾驶：** 任何失误都可能导致人身伤害，甚至更严重的后果。
*   **软件工程：** 编写生产级代码也具有高风险。一个错误可能导致安全漏洞、用户数据泄露（如数亿人的社会安全号），其后果是无法估量的。因此，软件工程师也应像自动驾驶工程师一样保持谨慎。

### “九个九”定律：可靠性的持续挑战

自动驾驶之所以耗时漫长，可以归结为“九个九”定律：每增加一个“九”（例如从90%的可靠性提升到99%，再到99.9%），都需要付出同样巨大的努力。当演示达到90%的成功率时，这仅仅是第一个“九”。之后还需要第二个、第三个、第四个乃至更多的“九”。在特斯拉的五年工作中，团队可能只完成了两到三个“九”的迭代，而要实现完全的自动驾驶，还有更多的“九”需要去攻克。这就是为什么这类技术需要如此漫长的时间来发展。

本文探讨了AI技术发展中实现高可靠性的挑战，并以自动驾驶为例，深入剖析了通用软件工程与物理世界AI系统之间的异同，以及这些差异对AI部署速度和经济性的影响。

## AI部署的“九九归一”与演示的局限性

技术进步是一个“九九归一”的漫长过程，即需要不断迭代、修正和完善，才能达到所需的可靠性水平。这就是为什么许多技术研发耗时如此之久。

### 演示的误区

对于演示（demos），作者持谨慎态度。他认为演示往往并不能真实反映产品的实际情况，因为它们通常是专门构建的，而非在真实复杂环境中运行。即使是可交互的演示，也远未达到产品化的要求。当产品真正推向市场并接触现实世界时，会面临无数挑战，包括各种需要修补的异常行为。因此，每一次“九”的实现都代表着巨大的工作量。

### 安全关键领域：编程的挑战

作者指出，除非是进行相对简单、有趣的编程（如“bip coding”），否则对于涉及“关键安全领域”的软件，其复杂性和所需的可靠性与自动驾驶领域的要求异曲同工。这进一步印证了实现高可靠性所需的时间。

## 自动驾驶与通用软件安全的类比

有观点认为，软件所需的安全性保障与自动驾驶技术类似，因为两者失败的代价都极高。

### 失败成本的考量

*   **人类驾驶员的错误率：** 平均每40万英里或每七年犯一次重大错误。
*   **编码代理的挑战：** 如果一个编码代理必须在至少七年内不犯错误才能部署，其难度将大大增加。实际上，考虑到代码输出的频率，在实际时间（“挂钟时间”）上，可能远低于七年就会出现灾难性错误。
*   **通用软件工程的复杂性：** 自动驾驶只是人类活动中“数千种”任务中的一个垂直领域。而通用软件工程涉及的领域更广，面临的挑战表面更复杂。

### 对比的异议：LLMs的优势

然而，也有人对这种类比提出异议。他们认为：

*   **自动驾驶的耗时原因：** 很大一部分时间花在了构建鲁棒的基础感知、表示模型以及具备常识理解和泛化能力上，以便处理轻微“离分布”的情况（例如，看到有人挥手时能够理解并做出反应，而无需专门训练）。
*   **LLMs/VLMs提供的“免费”能力：** 当今的大型语言模型（LLMs）或视觉语言模型（VLMs）在这些方面提供了“免费”的能力，这意味着我们不再需要解决这些非常基础的表示问题。
*   **部署模式的转变：** 因此，将AI部署到不同领域，就像是将配备现有模型的自动驾驶汽车部署到不同的城市——这固然具有挑战性，但并非一个需要十年才能完成的任务。

### 对LLMs优势的质疑及自动驾驶的现实

作者对此并不完全认同，认为LLMs提供的“免费”能力以及其泛化程度仍有待商榷：

*   **LLMs的局限性：** 尽管LLMs可能提供更强的通用智能，但它们仍相当“脆弱”（fallible），存在许多理解上的空白，并非“开箱即用”的神奇泛化能力。
*   **自动驾驶的现状：** 自动驾驶技术远未达到完全成熟。
    *   **部署规模有限：** 即使是Waymo等领先公司，其部署的车辆数量仍然非常少。
    *   **经济效益问题：** 现有自动驾驶系统在经济上并不划算，投入巨大（包括资本支出和运营维护成本），未来实现盈利仍是一场艰苦的斗争。
    *   **“无人驾驶”的假象：** 许多看似无人驾驶的车辆背后，都有庞大且复杂的远程运营中心，配备人工操作员进行远程干预。人类并未真正被移除，只是被“转移到我们看不到的地方”。
    *   **跨环境挑战：** 从一个环境到另一个环境的部署仍然面临挑战（例如，Waymo无法覆盖城市的所有区域，尤其是在信号不佳的地区）。
*   **对Tesla的肯定：** 作者认为Tesla采取了更具可扩展性的自动驾驶方案，并认为该团队表现出色，其方法在未来将更具优势。
*   **历史视角：** 自动驾驶的发展历史并非仅仅十年，而是从1980年代就开始了，且“终点尚未到来”，真正的“规模化自动驾驶”仍需时日（例如，人们不再需要驾照）。

## 部署AI的关键差异：经济性、延迟与社会影响

在探讨AI部署速度和早期价值时，需要考虑以下关键差异：

### 1. 经济性（资本支出与运营支出）

*   **自动驾驶：** 部署额外的自动驾驶车辆需要制造一辆全新的汽车，资本支出（CAPEX）极高。
*   **基于LLMs的知识工作：** 部署额外的模型副本虽然有额外成本，但单次会话的运营支出（OPEX）相对较低。AI的成本可以摊销到训练运行中。与构建一辆新车相比，其部署经济性要优越得多。

### 2. 延迟要求与模型规模

*   **自动驾驶：** 对延迟有严格要求（毫秒级响应），这通常需要庞大的模型（如数千万参数）。
*   **LLMs知识工作：** 虽然对知识工作而言，可能没有自动驾驶那般严苛的延迟要求，但要实现大规模的知识工作，仍然需要巨大的计算资源，这可能会带来实际的延迟问题。

### 3. “比特”与“物理世界”的本质差异

*   **比特的优势：** 数字化的“比特”比任何接触物理世界的东西都容易处理一百万倍。比特可以完全改变，并以极快的速度任意重组。这预示着该领域将有更快的适应和发展。

### 4. 其他社会与监管考量

除了技术和经济因素，AI的广泛部署还涉及：

*   **社会接受度：** 社会如何看待AI？
*   **法律法规：** 法律框架如何适应？
*   **保险责任：** 保险公司如何处理？
*   **层出不穷的问题：** 就像人们会用锥桶阻碍Waymo一样，在AI领域也会出现类似的意外情况。谁将扮演“隐形的远程操作员”角色？

作者认为，自动驾驶是一个很好的类比，可以从中借鉴经验。但需要深入思考，在通用AI部署中，什么是“锥桶”的等价物，什么是“隐藏的远程操作员”的等价物。

### AI计算资源过度建设的探讨

**主持人提出疑问：**
*   汽车上的“锥体”（cone）的等价物是什么？远程操作工作者（teleoperating worker）的等价物又是什么？他们仿佛隐匿起来，几乎包含了所有方面。
*   关于当前AI计算资源建设的观点：如果按照目前的发展速度，全球可用的计算能力将在未来一两年内增长10倍，甚至在本十年末可能增长超过100倍。但如果AI的实际应用量低于一些人天真预测的水平，这是否意味着我们正在过度建设计算资源？或者，这是否是一个独立的问题？

**嘉宾回应：**
*   这有点像历史上铁路建设或类似事件的情况。
*   噢，抱歉，也许更像电信行业，对吧？就像互联网基础设施在真正大规模应用（大约十年后才实现）之前就已铺设完毕，这在1990年代末造成了电信业的泡沫，就是那种感觉。是的。

### 对AI发展前景的看法：务实中的乐观

*   **表面的悲观与内在的乐观**：我在这里听起来可能显得非常悲观，但实际上我是一个乐观主义者。我认为这项技术是可行的，而且是可控的。我之所以听起来悲观，只是因为在我的Twitter时间线上看到了许多让我无法理解的内容。
*   **过度宣传的动因**：我认为这些现象背后有诸多原因，其中很大一部分是出于募资、吸引注意力和在互联网上将注意力转化为金钱等激励机制。所以我认为有很多这样的情况，而我只是在对此做出反应。
*   **对技术进步的信念**：我整体上对技术发展持非常乐观的态度。我相信我们能够克服所有挑战，并且已经取得了飞速的进步。
*   **关于计算资源过剩的驳斥**：
    *   我并不认为目前存在计算资源过度建设的情况。
    *   我相信我们将能够消化掉在我看来正在建设中的所有计算能力。
    *   因为我确实认为，例如Cloud Code或Open Codex等技术，在一年前甚至还不存在，对吧？这是奇迹般的技术。
    *   考虑到ChatGPT等产品已经展现出的巨大需求，未来对计算资源的需求无疑将是巨大的。
*   **校准预期的重要性**：
    *   我仅仅是对那些人们反复且错误地提及的“快速时间表”做出反应。
    *   在我从事AI领域的15年里，我曾多次听到许多备受尊敬的人士不断做出错误的预测。
    *   我希望能够正确地校准对AI发展的预期，因为这其中涉及到地缘政治等方面的复杂影响。我不想让人们在这方面做出错误判断。
    *   我们必须立足于技术的真实能力，区分其能与不能。

### 致力于人类福祉：教育项目Eureka

**主持人提问：**
*   我们可以谈谈Eureka项目中的教育方面吗？
*   一种做法是再创办一个AI实验室来解决这些问题。你现在在做什么？
*   为什么不专注于AI研究本身呢？

**嘉宾回应：**
*   我想，我对此的看法是，我感到AI实验室正在做的事情具有某种程度的确定性。我也能在那方面提供帮助，但我不确定我是否能带来独特的、实质性的改进。
*   **核心担忧**：我个人最大的担忧是，许多AI进展最终会发生在“人类之外”，导致人类被剥夺权力。
*   **关注人类命运**：我关心的不仅仅是AI未来完全自主构建的戴森球等宏大愿景，我更关心人类的命运。
*   **人类在未来的定位**：我希望人类在未来能够过得更好。我感觉在这方面，我能够独特地贡献更大的价值，远超在某个前沿实验室做出的增量改进。
*   **警示与愿景**：我最害怕的场景是电影《机器人总动员》（Wall-E）或《白痴联盟》（Idiocracy）中所描绘的，人类在其中被边缘化。我希望人类在未来能够显著提升生活质量。
*   **教育作为解决方案**：因此，我认为通过教育，我们能够实现这一目标。

### Eureka项目：打造“星际舰队学院”

**主持人提问：**
*   那么，你目前在Eureka项目上具体做了什么？

**嘉宾回应：**
*   噢，是的。Eureka正在尝试建立，我认为最简单的描述就是，我们正在打造一个“星际舰队学院”（Starfleet Academy）。
*   我不知道你是否看过《星际迷航》（Star Trek），我没有。
*   （主持人补充解释）好的，星际舰队学院是一个精英机构，专注于前沿技术，比如建造宇宙飞船，并培养出能驾驶这些飞船的学员。
*   **Eureka的愿景**：我将其设想为一个专注于技术知识的精英机构，一个非常现代、顶尖的教育平台。

### 关于技术和科学内容教学的思考

**主持人提问：**
*   我有一些关于你如何教授技术或科学内容的问题，因为你在这方面是世界级的大师。
*   我很好奇你如何看待你在YouTube上发布的内容，以及Eureka项目是否有不同的教学理念？

**嘉宾回应：**
*   **AI对教育的根本性变革**：关于Eureka，我发现教育的一个非常引人入胜之处在于，我认为它将随着AI的出现而发生根本性的变化，并在某种程度上需要被重新调整和改变。
*   **当前AI导师的局限性**：
    *   我承认我们仍处于早期阶段。许多人会尝试显而易见的做法，比如利用大型语言模型（LLM）来回答问题或进行基本提示式交互。
    *   尽管这有帮助，但对我来说仍然感觉不够完善。我希望能够做得更专业，而当前AI的能力尚未达到我期望的水平。
    *   我期望的是一种真正的“导师体验”。
*   **个人导师经验的启发（韩语学习案例）**：
    *   一个突出的例子是我最近学习韩语的经历。我经历过自学、参加韩国小班教学，然后转为一对一的导师指导。
    *   **一对一导师的卓越之处**：
        *   我有一位非常优秀的导师。她在很短的对话后就能立即理解我的学生水平，了解我的已知和未知。
        *   她能准确地提出问题，探究我的“世界模型”。目前任何LLM都无法百分之百做到这一点，甚至差得很远。
        *   一旦她理解了我的情况，她就能精准地提供我当前能力水平所需的所有内容。
        *   我需要始终被适当挑战，既不能太难也不能太简单。优秀的导师非常擅长提供恰到好处的学习内容。
        *   我感到学习的唯一限制是我自己。我总能获得完美的信息，不再是找不到知识，也不是解释不清楚等问题。
*   **对自动化导师的期望**：我希望为人们提供这样的体验。如何实现自动化？
*   **当前AI能力的不足**：
    *   对于当前AI能力，你无法实现这种自动化。
    *   这也是为什么我认为现在并非构建这种AI导师的“正确时机”。它仍然是一个有用的产品，很多人会去构建，但我感觉标准太高，而能力尚未到位。
    *   不过，我也会说ChatGPT本身就是一个非常有价值的教育产品。
    *   但我的导师经历让我认识到这个门槛有多高，以至于我一度觉得“我根本不可能构建出这样的东西”。
    *   任何有过优秀导师经验的人都会质疑，这要如何通过技术实现？
*   **等待技术成熟**：我正在等待那种能力的到来。
*   **AI咨询的经验借鉴**：
    *   在很多情况下，我在AI行业中，例如为计算机视觉提供AI咨询时，我的价值在于告诉公司“不要使用AI”。
    *   当他们描述一个问题时，我不会说“用AI”，而是“不要用AI”。这就是我的附加价值。
    *   同样的，在教育领域，我认为对于我心目中的愿景，时机尚未成熟，但时机终将到来。
    *   不过目前，我正在构建一个可能看起来更“传统”的产品，它包含物理和数字组件。但我认为未来的蓝图清晰可见。

### Eureka近期目标：首个课程的发布

**主持人提问：**
*   你是否愿意透露一下，你希望今年或明年能发布什么？

**嘉宾回应：**
*   我正在构建第一个课程，我希望它能成为一个非常出色、最先进的课程。
*   我希望它能成为学习AI的首选目的地，因为那是我最熟悉的领域。
*   因此，我认为这是一个很好的首发产品，目标是做到卓越。
*   你之前简单提到的“Nano Chat”是我正在开发的课程“LLM 101n”的一个核心项目。
*   这是其中一个非常重要的部分，但我还需要构建大量其他内容。

以下是原文改写后的高质量中文Markdown文章：

---

## LLM 101 课程构建与教育理念

目前正在着手构建一门名为 "LLM 101" 的课程。这项工作不仅涉及核心内容的开发，还需要：

*   **中间环节建设：** 大量中间材料的开发。
*   **团队组建：** 招聘一支由助教（TA）等组成的团队。
*   **课程整体构建：** 负责整个课程的全面建设。

### 对教育的深刻理解：构建知识的“斜坡”

在谈到教育时，人们往往只关注知识传播的“软性”层面。然而，我将其视为一个“非常困难的技术问题”，即构建通往知识的“斜坡”（Ramps to Knowledge）。

**NanoChat：一个高效的知识斜坡实例**

例如，NanoChat 就是一个通往知识的“斜坡”。它是一个极其简化的全栈项目，当人们浏览它时，能够学到大量东西。它提供了我所谓的“每秒尤里卡”（Eurekas per second），即每秒获得的理解量。我追求的正是这种大量的“每秒尤里卡”。

**教育的核心挑战：高效构建知识斜坡**

对我而言，教育是一个技术性问题：我们如何才能高效地构建这些知识斜坡，确保学习者永不卡壳？如何让所有材料既不过难也不过易，使学习者能够持续进步？

### 助教（TA）与人工智能在教育中的角色

**短期展望：人机协作**

*   **当前能力边界：** 虽然很多人可能会倾向于直接询问ChatGPT等工具来学习，但目前它们在教授复杂主题（如AI本身）时仍力有不逮，输出的知识可能较为零散。
*   **NanoChat的价值：** NanoChat作为一个有用的中间点，目前AI尚无法独立完成。
*   **AI作为辅助：** 我依然在与AI协作创建这些材料，AI在其中发挥了非常基础且有益的作用。

**与CS231N课程建设的对比**

我早期在斯坦福大学构建了CS231N课程（第一门深度学习课程，后来非常流行）。与当年相比，现在构建LLM 101课程的体验截然不同，这得益于现有大型语言模型的赋能。它们极大地加快了材料制作速度，承担了许多“无聊”的工作。因此，课程开发速度快了很多，且课程中融入了LLM技术。

尽管如此，AI尚未达到能够进行创造性内容创作的阶段，我本人仍然深度参与其中，负责创意部分。

**长期演进：AI角色的深化**

*   **未来瓶颈：** 几年后，在“尤里卡”模式下，主要的瓶颈将是找到各个领域的专家，将他们的理解转化为这些“斜坡”。
*   **教师与AI的合作：** 最初阶段，需要聘请教师团队与AI紧密合作，共同构建最先进的课程。
*   **AI助教：** 随着时间推移，一些助教（TA）的角色可能会由AI承担，负责回答学生的基础问题。
*   **教师的核心价值：** 然而，教师仍将是课程整体架构设计和内容适配性的关键。
*   **最终愿景：** 也许在未来的某个时刻，AI在课程设计方面将超越人类，甚至我的作用也会变得不那么关键。但这还需要时间。

### Starfleet Academy的未来规划

**关于教师团队：**

我计划聘请专业的教师团队，因为在某些领域我并非专家。这是为学生提供最先进学习体验的唯一途径。虽然我可能会在AI领域继续工作一段时间，但我对当前AI能力的设想比很多人预期的更具“传统”色彩。

**物理与数字课程：**

在构想Starfleet Academy时，我设想它是一个物理实体机构。同时，在其之下会有一个数字课程产品，虽然无法提供与亲身到校、全职学习并确保完全理解材料相同的最先进体验，但这种数字产品（包含在线材料和LLM辅助）具有更广泛的可及性，可以触及全球数十亿人。

### AI时代的教育与再教育

**变革中的大学教育**

这就像从第一性原理出发，为当下可用的工具重新设计大学教育。它旨在筛选出那些真正有动力、有兴趣深入学习材料的人。

**就业与技能重塑**

我们不仅需要教育，更需要再教育，因为就业市场将发生巨大变化。例如，现在很多人都在寻求AI方面的技能提升，这使得AI成为一个非常重要的教学领域。

**AGI前后的教育动机**

*   **AGI之前：** 学习动机非常简单直接——赚钱。这是当今行业中赚钱的方式。
*   **AGI之后：** 动机将变得更有趣。如果一切都自动化了，人们无事可做，为什么还要去学校？
*   **“AGI前教育有用，AGI后教育有趣。”** 这就像今天人们去健身房一样。我们不再需要体力劳动来搬运重物，因为有机器代劳。但人们仍然去健身房，因为这很有趣、健康，还能保持体形。这是一种深层的心理和进化需求。

**学习的未来：娱乐化与便捷化**

我认为教育也将以类似的方式发展。你可能会像去健身房一样去上学。现在很多人不学习，因为学习很难。很多人会遇到障碍，虽然有些人能克服，但对大多数人来说这很难。但我认为这是一个可以解决的技术问题。这就像我的韩语家教所做的那样，她让学习变得更容易。

我认为这是可以实现和构建的。有人应该去建造它。这将使学习任何东西都变得轻而易举，令人向往，人们会因为它的简单而将其视为乐趣。如果我能拥有这样一个家教，能学习任何领域的知识，那学习会变得无比容易，人们会去学习，动机就像他们去健身房一样。

### 教育：是娱乐，还是赋能？

**对“失去的博弈”的思考**

你提出的问题是：在AGI之后，教育是作为娱乐和自我提升，还是作为人类掌控AI的手段？

我认为，从长远来看，这可能是一场“失去的博弈”。尽管如此，我坚信人类可以走得更远，我们才刚刚触及人类潜能的表面。人们之所以停滞不前，是因为学习材料要么太简单，要么太难。我认为人们能够取得更大的进步，例如轻松掌握五种语言，或者理解大学本科的所有基础课程。

**健身文化的类比：人类潜能的爆发**

现在我理解了这个愿景，它与健身文化有着完美的类比。一百年前，没有人会像现在这样身材健硕。但现在，通过系统的训练和举重，这种体型已经非常普遍。这表明，通过系统化的教育“训练”，人类的学习能力将得到极大的提升。

---

以下是原文的Markdown改写版本：

# 展望后AGI时代的人类潜能与学习范式

## 系统化训练的未来展望

当下，通过系统化训练（如健身房举重或马拉松训练）来获得人类在自然状态下不具备的能力已变得非常普遍。我们正在设想将这种系统训练的理念应用于更广泛的学习领域，使其变得更深入、更快速、更密集。

我隐含地寄希望于人性的永恒性。我认为追求这些能力的愿望将是人类固有的，人们会像千百年来一样，继续仰慕那些在各方面有所成就的人。历史上，在某些特定环境下——例如古希腊的贵族阶层，某种程度上可以被视为“后AGI”时代的微缩环境——人们会将大量时间用于智力或体能上的发展。这预示着未来人类仍会致力于在这些领域实现卓越。

## 对“瓦力”式未来的担忧与人类的“超凡化”

如果我的设想是错误的，我们最终走向一个像《瓦力》或《蠢蛋进化论》中描绘的未来，那将是一个糟糕透顶的结果。即使存在戴森球那样的宏伟技术成就，我也不会为此感到高兴，因为我真正关心的是人类的福祉。我希望每个人都能在某种意义上成为“超凡人类”。

但这引出了一个问题：在这样的世界中，人类是否还能通过自己的努力或认知来根本性地改变科技轨迹或影响重大决策？或许我们只是批阅AI提出的方案，而不再是凭借自己的发明创造或新设计来真正影响未来？

我认为，我们会经历一个过渡期，在此期间，如果我们能够深入理解事物，人类将能继续参与并推动进步。虽然从长远来看，这种“参与”可能会逐渐消失，但也许它会演变成一种新的“运动”。就像今天的举重运动员挑战极限一样，在认知时代，我们可能会看到一些人致力于将知识的掌握推向极致，甚至出现“知识奥林匹克”。

拥有完美的AI导师，我们可以达到前所未有的高度。我甚至觉得，今天的“天才”们也仅仅触及了人类心智潜能的皮毛。我非常喜欢这个愿景。

## 学习的内在驱动力与教学方法

我与这种愿景高度契合，因为我的工作要求我每周学习不同的新主题，所以我非常期待这样的未来。我也一样，很多人讨厌学校并想逃离，但我却非常喜欢学校，热爱学习。我一直读到博士，后来因为无法继续深造才转入业界。总的来说，我热爱学习，既是为了学习本身，也是因为学习是赋能、实现自我价值和生产力的途径。

关于在线课程，它们为何未能让每个人都无所不知？我认为关键在于其动机负载过重。学习者常常缺乏明确的入门路径，很容易卡住。如果能有一个真正优秀的“人类”导师（或其AI等效物），这将极大地解决动机问题。

当学习过程设计得当，学习本身是令人愉悦的。相反，如果在某件事上投入了时间却没有结果，或者因为内容太简单或太难而感到无聊，这都会带来负面反馈，让人感到沮丧。

我认为要实现理想的学习体验，这是一个技术问题。在一段时间内，将是AI与人类的协作模式，而最终可能完全由AI主导，这还有待观察。

## 源自物理学的教学哲学：抽象与建模

如果让我给其他领域的教育者一些建议，尤其是在那些难以通过编程等技术手段测试理解力的领域，我会提供以下几点：

这是一个相当宽泛的话题，我自觉有10到20个半意识的技巧。但从高层次来看，我的教学方法深受我的物理学背景影响。我非常享受我的物理学学习经历，并认为每个人都应该在早期学校教育中学习物理。早期教育的目的不是为了将来在行业中积累知识或记忆，而是为了“启动大脑”，而物理学能以独特的方式最好地实现这一点。

物理学训练大脑的方式，例如构建模型和抽象、理解事物存在第一、第二、第三、第四阶近似，以及如何在观察到高度“嘈杂”的系统时，仍能抽象出其根本频率，这些都极具价值。当物理学家在课堂上说“假设有一只球形奶牛……”时，虽然大家会笑，但这实际上是一种非常了不起且可推广的思维方式。它教会我们如何从复杂现象中提取核心要素。

例如，有一本名为《尺度》的优秀书籍，作者是一位物理学家，探讨了生物学。这本书非常值得一读。它展示了如何通过近似，推导出动物的尺度定律，比如心跳速率与动物体型大小的关系，以及热量散失（与表面积呈平方关系）与热量产生（与体积呈立方关系）之间的关系。

我认为物理学家拥有一套正确的认知工具来解决世界上的问题。因此，基于这种训练，我总是试图找出任何系统或事物的“第一阶”或“第二阶”项。当我在观察一个系统或一个事物时，我头脑中有一团相互交织的思想或知识网，我试图找出真正重要的东西是什么，即第一阶组成部分。我如何简化它？如何用一个简单的模型来展示这个事物在实际中是如何运作的？然后，我再逐步添加其他项。

## 案例分析：Micrograd——揭示核心原理

我的一个名为“Micrograd”的代码仓库很好地诠释了这种思想。Micrograd是一个用不到100行Python代码实现反向传播的库。你可以用它通过简单的加法、乘法等乐高积木般的运算来构建神经网络，然后建立计算图，进行前向和后向传播以获取梯度。

这是所有神经网络学习的核心。Micrograd这100行易于理解的Python代码，包含了你理解神经网络训练所需的一切。其他的一切都只是效率问题。

是的，其他的一切都只是效率。当然，为了效率，我们需要做大量工作：处理张量、布局、步幅、确保核函数正确协调内存移动等等。但从核心的智力层面来看，神经网络训练的关键就是Micrograd所展示的。它通过递归应用链式法则来推导梯度，从而优化任何可微分函数。

我喜欢找到这些“小而核心”的元素，并以清晰的方式呈现出来。教育在我看来是智力上最有趣的事情，因为它涉及将一团复杂的知识理顺，以一种循序渐进的方式展现，让每个概念都建立在前一个概念之上。这种知识的“解缠结”本身就是一项非常引人入胜的认知任务。我个人非常喜欢做这件事，因为它让我着迷于以某种方式组织事物，这可能也帮助了我。

这也能极大地激发学习体验。例如，我的Transformer教程从最简单的二元语法（biogram）查找表开始。

# 深入理解与高效学习：从Transformer教学到克服“知识的诅咒”

本文探讨了高效的教学方法，特别是如何循序渐进地讲解复杂概念如Transformer，并深入分析了专家在传授知识时常遇到的“知识的诅咒”现象，最后提供了提升个人学习和解释能力的实用策略。

## 一、教学策略：从基础到Transformer的循序渐进

一个优秀的教程会以一种巧妙的方式引导学习者，让他们在理解每个组件的动机和作用时，逐步掌握复杂系统。

### 1. 以查找表为例的Biogram教学

Transformer的教程可以从Biogram开始。Biogram本质上就是一个查找表，例如，给定“当前词”，或者“前一个词”，就能查到“下一个词”。这种基于查找表的简单机制构成了其核心。这种从查找表开始，再逐步过渡到Transformer的教学方式非常巧妙。

### 2. 启发式教学：驱动学习的“痛点-解决方案”模式

教程中的每个部分都被赋予了明确的动机：
*   “为什么你要添加这个？”
*   “为什么你要添加下一个组件？”

这种方式让学习者不仅记住了一个“注意力机制”的公式，更重要的是，他们理解了每个组件为何存在、解决了什么问题。这就像在提出解决方案之前，先呈现“痛点”，这种教学方式非常聪明。

一个好的教学过程应该引导学生经历这样的进步，通过提出“你会如何解决这个问题？”来不断鼓励学生思考，而不是直接给出答案。在学生尝试解决问题之前就提供解决方案是低效且不利于学习的。

*   **鼓励自主思考：** 如果学生有机会自己尝试解决问题，他们会对“行动空间”和“目标”有更深入的理解，例如，为什么只有某个特定的行动才能达成目标。
*   **最大化知识获取：** 这种自主尝试的过程能让学生在获得解决方案时产生更强的领悟，从而最大化每新增一个知识点所带来的理解量。

## 二、专家为何难以解释？“知识的诅咒”

为什么领域内的真正专家往往不擅长向初学者解释他们的专业知识？

### 1. “知识的诅咒”现象

这种现象被称为“知识的诅咒”或“专业知识的诅咒”。这是一个真实存在的现象，即使是讲解者本人也曾深受其困扰，尽管他努力避免。专家们往往会把某些事物视为理所当然，难以站在初学者的角度去理解他们的困惑。这种普遍存在的现象，也时常发生在专家自己身上。

## 三、提升解释与学习效率的策略

为了克服“知识的诅咒”并提高学习效果，可以采用以下策略：

### 1. 利用AI辅助学习与教学反馈

一个非常有用的方法是利用AI工具（如ChatGPT）来帮助理解和教学反馈：
*   **模拟初学者提问：** 曾有人向我展示一篇生物学论文，我立刻产生了许多“糟糕”的问题。我将论文放入ChatGPT的上下文窗口，让AI帮助我提出并梳理这些简单的问题。
*   **向作者提供反馈：** 随后，我将与ChatGPT的对话记录分享给了论文的作者（或相关研究人员）。我感觉，如果作者能看到我这些“愚蠢的问题”，或许能帮助他们在未来更好地解释相关概念。
*   **鼓励对话分享：** 我也希望人们能分享他们与ChatGPT关于我所创建内容的“愚蠢对话”，这能帮助我再次设身处地地站在初学者的角度思考。

### 2. 口头交流的清晰性与精确性

另一个效果惊人的技巧是口头交流：
*   **非正式解释的优势：** 当一个人撰写论文、博客文章或公告时，通过非正式的口头叙述（例如，在午餐时解释）其内容，往往比书面形式更易懂、更准确，甚至在某种意义上更具科学性。
*   **书面表达的偏差：** 人们在书面解释时，倾向于使用最抽象、充满行话的方式，并且在阐述核心思想之前会进行冗长的铺垫。
*   **一对一沟通的直接性：** 然而，与人一对一沟通时，你会被迫直奔主题，直接说明核心内容。
*   **亲身体验：** 在我攻读博士学位期间，我常常发现，阅读一篇论文并努力理解其内容后，如果在会议上遇到作者，在喝啤酒时问他们论文是关于什么的，他们会用三句话完美地概括论文的精髓，让我立刻明白，而之前我甚至不需要读完论文。这种口语化的表达方式总能完美地捕捉核心思想，让人不禁想：“为什么论文本身不能这样写？”

### 3. 对学生的学习建议

对于学生而言，在没有像“卡帕西”这样擅长阐释想法的老师时，阅读论文或书籍，有哪些学习策略可以采纳？

*   **学习是一个“痛苦”的过程：** 学习本身是一个需要付出努力的过程，没有所谓的“独门秘诀”。
*   **深度学习与广度学习的平衡：** 我发现一个有用的方法是平衡按需学习（深度）与广泛学习（广度）。
    *   **按需学习（Depth-wise Learning）：** 这种学习方式围绕着一个你渴望完成并能获得奖励的项目展开，它非常有效。
    *   **广泛学习（Breadth-wise Learning）：** 这种方式通常在学校教育中常见，教授一些“你以后会用到”的知识，但学生往往是出于“我猜我需要它”的心态去学习。
    *   我更偏爱那种在实践中通过完成任务获得回报，从而按需学习的方式。

*   **“解释给他人听”的利他式学习：** 另一个极其有益的方法是“解释给他人听”。这或许是教育中更具利他主义的一面，因为向他人解释事物是深入理解知识的绝佳途径。
    *   **发现理解盲区：** 我经常发现，如果我不能真正理解某个概念，就无法清晰地解释它。在尝试解释的过程中，我可能会意识到：“实际上，我根本不理解这个。” 认识到这一点虽然令人沮丧，但它迫使你回到源头，确保自己真正理解，从而填补理解上的空白，并将矛盾之处理顺。
    *   **知识的内化与操控：** 我喜欢不断重新解释事物，也认为其他人应该更多地这样做。这迫使你去操控知识，确保在解释时你真正理解自己在说什么。

## 四、结语与致谢

这是一个非常棒的结束语。

感谢Andre，这次对话非常精彩。

---

**听众须知：**

大家好，希望你们喜欢这期节目。如果你喜欢，最能帮助我们的就是将它分享给其他可能感兴趣的朋友。同时，如果你在任何平台上收听，留下评分或评论也会非常有帮助。如果您有兴趣赞助我们的播客，可以通过dwarcash.com/advertise联系我们。下次节目再见！
